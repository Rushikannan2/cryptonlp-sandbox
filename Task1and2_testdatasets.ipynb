{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 313311,
     "status": "ok",
     "timestamp": 1751265285071,
     "user": {
      "displayName": "VT Rushi Kannan",
      "userId": "09227156192590121303"
     },
     "user_tz": -330
    },
    "id": "XNW2QLTSfPda",
    "outputId": "ec56ce11-2808-4b2f-b4bd-3cd0711334db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Reddit Inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Twitter Inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç YouTube Inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All predictions complete. Zip the 4 CSVs for submission.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === SETTINGS ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"microsoft/deberta-v3-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "max_length = 256\n",
    "batch_size = 32\n",
    "\n",
    "# === MODEL PATHS ===\n",
    "level1_paths = [f\"/content/drive/MyDrive/FIRE/run_20250628_034630/models/level1_fold{i}.pth\" for i in range(1, 6)]\n",
    "level2_paths = [f\"/content/drive/MyDrive/FIRE/run_20250629_090953/models/level2_fold{i}.pth\" for i in range(1, 6)]\n",
    "level3_paths = [f\"/content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold{i}.pth\" for i in range(1, 6)]\n",
    "\n",
    "# === DATASET ===\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "\n",
    "# === MODEL LOADER ===\n",
    "def load_model(path, num_labels):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# === ENSEMBLE INFERENCE ===\n",
    "def ensemble_predict(model_paths, dataset, num_labels):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    all_probs = np.zeros((len(dataset), num_labels))\n",
    "    for path in model_paths:\n",
    "        model = load_model(path, num_labels)\n",
    "        fold_probs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                logits = model(**batch).logits\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "                fold_probs.append(probs)\n",
    "        fold_probs = np.concatenate(fold_probs, axis=0)\n",
    "        all_probs += fold_probs\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    avg_probs = all_probs / len(model_paths)\n",
    "    preds = np.argmax(avg_probs, axis=1)\n",
    "    return preds\n",
    "\n",
    "# === COMMON INFERENCE FUNCTION ===\n",
    "def hierarchical_predict(df, input_texts):\n",
    "    dataset = TextDataset(input_texts)\n",
    "    level1_preds = ensemble_predict(level1_paths, dataset, num_labels=3)\n",
    "    level2_preds = [\"\" for _ in df.index]\n",
    "    level3_preds = [\"\" for _ in df.index]\n",
    "\n",
    "    idx_level2 = [i for i, p in enumerate(level1_preds) if p == 2]\n",
    "    texts_level2 = [input_texts[i] for i in idx_level2]\n",
    "    dataset_level2 = TextDataset(texts_level2)\n",
    "    level2_subset_preds = ensemble_predict(level2_paths, dataset_level2, num_labels=3)\n",
    "    for j, i in enumerate(idx_level2):\n",
    "        level2_preds[i] = int(level2_subset_preds[j])\n",
    "\n",
    "    idx_level3 = [i for i in idx_level2 if level2_preds[i] == 0]\n",
    "    texts_level3 = [input_texts[i] for i in idx_level3]\n",
    "    dataset_level3 = TextDataset(texts_level3)\n",
    "    level3_subset_preds = ensemble_predict(level3_paths, dataset_level3, num_labels=4)\n",
    "    for j, i in enumerate(idx_level3):\n",
    "        level3_preds[i] = int(level3_subset_preds[j])\n",
    "\n",
    "    return level1_preds, level2_preds, level3_preds\n",
    "\n",
    "# === REDDIT ===\n",
    "reddit = pd.read_csv(\"/content/drive/MyDrive/FIRE/CRYPTO_REDDIT_TEST.csv\").fillna(\"\")\n",
    "reddit_texts = reddit[\"MAIN\"].astype(str).tolist()\n",
    "print(\" Reddit Inference...\")\n",
    "level1, level2, level3 = hierarchical_predict(reddit, reddit_texts)\n",
    "reddit[\"level 1\"] = level1\n",
    "reddit[\"level 2\"] = level2\n",
    "reddit[\"level 3\"] = level3\n",
    "reddit = reddit[[\"title\", \"selftext\", \"MAIN\", \"level 1\", \"level 2\", \"level 3\"]]\n",
    "reddit.to_csv(\"/content/drive/MyDrive/FIRE/crypto_test_reddit.csv\", index=False)\n",
    "\n",
    "# === TWITTER ===\n",
    "twitter = pd.read_csv(\"/content/drive/MyDrive/FIRE/CRYPTO_TWITTER_TEST.csv\").fillna(\"\")\n",
    "twitter_texts = twitter[\"Text\"].astype(str).tolist()\n",
    "print(\" Twitter Inference...\")\n",
    "level1, level2, level3 = hierarchical_predict(twitter, twitter_texts)\n",
    "twitter[\"Level 1\"] = level1\n",
    "twitter[\"Level 2\"] = level2\n",
    "twitter[\"Level 3\"] = level3\n",
    "twitter = twitter[[\"Text\", \"Level 1\", \"Level 2\", \"Level 3\"]]\n",
    "twitter.to_csv(\"/content/drive/MyDrive/FIRE/crypto_test_tweet.csv\", index=False)\n",
    "\n",
    "# === YOUTUBE ===\n",
    "youtube = pd.read_csv(\"/content/drive/MyDrive/FIRE/CRYPTO_YOUTUBE_TEST.csv\").fillna(\"\")\n",
    "youtube_texts = youtube[\"MAIN\"].astype(str).tolist()\n",
    "print(\" YouTube Inference...\")\n",
    "level1, level2, level3 = hierarchical_predict(youtube, youtube_texts)\n",
    "youtube[\"Level1\"] = level1\n",
    "youtube[\"Level2\"] = level2\n",
    "youtube[\"Level3\"] = level3\n",
    "youtube = youtube[[\"comment_id\", \"MAIN\", \"Level1\", \"Level2\", \"Level3\"]]\n",
    "youtube.to_csv(\"/content/drive/MyDrive/FIRE/crypto_test_youtube.csv\", index=False)\n",
    "\n",
    "print(\" All predictions complete. Zip the 4 CSVs for submission.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 222510,
     "status": "ok",
     "timestamp": 1751266714856,
     "user": {
      "displayName": "VT Rushi Kannan",
      "userId": "09227156192590121303"
     },
     "user_tz": -330
    },
    "id": "nUQOZaG_heqi",
    "outputId": "eea09c9f-364b-44e5-8317-763bcd052767"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîç Predicting relevance: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 198/198 [02:43<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved QnA predictions to: /content/drive/MyDrive/FIRE/crypto_test_qna.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2ForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === Paths\n",
    "TEST_PATH = \"/content/drive/MyDrive/FIRE/CRYPTO_QnA_TEST.csv\"\n",
    "MODEL_DIR = \"/content/drive/MyDrive/FIRE/run_20250629_121221/fold_outputs/fold_2/best_model\"\n",
    "OUTPUT_PATH = \"/content/drive/MyDrive/FIRE/crypto_test_qna.csv\"  # Final output path\n",
    "\n",
    "# === Load Test Data\n",
    "df = pd.read_csv(TEST_PATH).fillna(\"\")\n",
    "\n",
    "# === Combine fields for input\n",
    "def combine_text(row):\n",
    "    return f\"{row['title']} [SEP] {row['selftext']} [SEP] {row['MAIN']} [SEP] {row['comment_body']}\"\n",
    "df['combined'] = df.apply(combine_text, axis=1)\n",
    "\n",
    "# === Load Model & Tokenizer\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_DIR)\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "\n",
    "# === Dataset Class\n",
    "class QnADataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "\n",
    "# === DataLoader\n",
    "dataset = QnADataset(df['combined'].tolist(), tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "# === Inference\n",
    "preds = []\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(loader, desc=\"Predicting relevance\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        preds.extend(predictions.cpu().tolist())\n",
    "\n",
    "# === Save to correct CSV format\n",
    "df[\"relevance\"] = preds\n",
    "df.drop(columns=[\"combined\"], inplace=True)\n",
    "\n",
    "# Ensure correct columns & order\n",
    "final_cols = [\"title\", \"selftext\", \"MAIN\", \"comment_body\", \"relevance\"]\n",
    "df = df[final_cols]\n",
    "\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Saved QnA predictions to: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 972,
     "status": "ok",
     "timestamp": 1751267013600,
     "user": {
      "displayName": "VT Rushi Kannan",
      "userId": "09227156192590121303"
     },
     "user_tz": -330
    },
    "id": "h4FyAPYgpbte",
    "outputId": "194be6a2-9337-4ded-edc8-f9ddc887c7e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created ZIP for submission at: /content/drive/MyDrive/FIRE/rushikannan.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "#  CSV file paths\n",
    "csv_files = {\n",
    "    \"crypto_test_reddit.csv\": \"/content/drive/MyDrive/FIRE/crypto_test_reddit.csv\",\n",
    "    \"crypto_test_tweet.csv\": \"/content/drive/MyDrive/FIRE/crypto_test_tweet.csv\",\n",
    "    \"crypto_test_youtube.csv\": \"/content/drive/MyDrive/FIRE/crypto_test_youtube.csv\",\n",
    "    \"crypto_test_qna.csv\": \"/content/drive/MyDrive/FIRE/crypto_test_qna.csv\"\n",
    "}\n",
    "\n",
    "#  Output zip path\n",
    "zip_path = \"/content/drive/MyDrive/FIRE/rushikannan.zip\"\n",
    "\n",
    "#  Create zip file\n",
    "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "    for name, path in csv_files.items():\n",
    "        zipf.write(path, arcname=name)\n",
    "\n",
    "print(f\" Created ZIP for submission at: {zip_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPhPEmvqR0Ad8pCQSb/R/CQ",
   "gpuType": "T4",
   "mount_file_id": "1c5ZqcKeUtL7Vb373ZKD71f20Q7ARC4BY",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
