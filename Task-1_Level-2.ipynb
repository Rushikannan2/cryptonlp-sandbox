{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254,
     "referenced_widgets": [
      "44b39c07c5744964a314ed77b98b3894",
      "d86da9084fa54fe694d8d41b78155214",
      "1232078a2d4845dcb432920da5664218",
      "ea05048187ae4a44a4204e4d18221049",
      "4b252fbffce84e4aa6b222fe45e27162",
      "64e7ef31555846a1bbcea81036ea3284",
      "bb9423f3566549d78a189d9d78ca8e52",
      "5efb16758bf94d919ce318805a32c544",
      "774475469caa4cb59512ab45f04813cf",
      "5f6def2df2f2419fa56178bbb45b2675",
      "7246485adddc452aa3bc81456295d13b",
      "ae49923797374fba96a9bc37da204683",
      "cf4245154e304e4897d9c23a2da42c52",
      "fa75f857b8df4d07a3cdea400b34ec3a",
      "0b0d2a52dd794e2280032f317eaba9d8",
      "4ac885be819645bbbc2003a1fcf7fa02",
      "d0b2313047bc475c9186dc687093a533",
      "27908a38c6ff4884aa33a2eaa7cdb71e",
      "34446b23177146c5afeb1efd254066f1",
      "6dfbd178ff61410494469a70bf864108",
      "7076bf77fdec4d97981f57cd101e2032",
      "58b95719dce74cf4bb37337e836c472c",
      "2c7e3c71aa9c4eb5b926f8f6dde61830",
      "f63e02771722410e878badaf7fb1d24a",
      "93ed2c04503445939d47a5b9319325a4",
      "7e45c21d654a4e54b0dc1a998a8e6832",
      "d8c89aeb8e224fdf9b7a370aba832890",
      "b123e09adc9f4f1583c2db2761128873",
      "909110ec77d64d1fa567a315c095577c",
      "8c77483b0cb0454c990be1ddce7a2f30",
      "fb0a0b7a65f042778dba4845a9673bc4",
      "826ae83fb28047a2aae81fa7645dde57",
      "8d78c961dec34ac59470d8ab74af79a1"
     ]
    },
    "executionInfo": {
     "elapsed": 38699,
     "status": "ok",
     "timestamp": 1751186201005,
     "user": {
      "displayName": "Klh edu",
      "userId": "03499339510403364543"
     },
     "user_tz": -330
    },
    "id": "BsjOYebcv1Qm",
    "outputId": "06f9f8a5-85fb-4d1a-c232-a38339d88476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b39c07c5744964a314ed77b98b3894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae49923797374fba96a9bc37da204683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7e3c71aa9c4eb5b926f8f6dde61830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------- Imports --------------------\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import mode\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import (\n",
    "    DebertaV2Tokenizer,\n",
    "    DebertaV2ForSequenceClassification,\n",
    "    get_scheduler\n",
    ")\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "VAL_BATCH_SIZE = 32\n",
    "MAX_LENGTH = 128\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 4e-5\n",
    "PATIENCE = 1\n",
    "MODEL_NAME = 'microsoft/deberta-v3-small'\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/FIRE/outputs\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------- Seed Setup --------------------\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------- Tokenizer --------------------\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"tokenizer\"))\n",
    "\n",
    "# -------------------- Dataset --------------------\n",
    "class CryptoDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([b['input_ids'] for b in batch]),\n",
    "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
    "        'labels': torch.stack([b['labels'] for b in batch]),\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------- Confusion Matrix --------------------\n",
    "def plot_confusion_matrix(labels, preds, classes, title, save_path):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------- Contrastive Supervision --------------------\n",
    "def apply_contrastive_supervision(features, labels, temperature=0.1):\n",
    "    features = F.normalize(features, dim=1)\n",
    "    similarity_matrix = torch.matmul(features, features.T)\n",
    "    labels = labels.contiguous().view(-1, 1)\n",
    "    mask = torch.eq(labels, labels.T).float().to(features.device)\n",
    "\n",
    "    logits = similarity_matrix / temperature\n",
    "    logits_mask = torch.ones_like(mask) - torch.eye(mask.size(0), device=mask.device)\n",
    "    mask = mask * logits_mask\n",
    "\n",
    "    exp_logits = torch.exp(logits) * logits_mask\n",
    "    log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-9)\n",
    "    mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-9)\n",
    "\n",
    "    loss = -mean_log_prob_pos.mean()\n",
    "    return loss\n",
    "\n",
    "# -------------------- Training Losses --------------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, weight=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        pt = torch.exp(logpt)\n",
    "        logpt = (1 - pt) ** self.gamma * logpt\n",
    "        loss = F.nll_loss(logpt, target, weight=self.weight, reduction=self.reduction)\n",
    "        return self.alpha * loss\n",
    "\n",
    "def dice_loss(logits, targets, smooth=1):\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=logits.size(1)).float().to(logits.device)\n",
    "    intersection = (probs * targets_one_hot).sum(dim=0)\n",
    "    cardinality = probs.sum(dim=0) + targets_one_hot.sum(dim=0)\n",
    "    dice = (2. * intersection + smooth) / (cardinality + smooth)\n",
    "    return 1. - dice.mean()\n",
    "\n",
    "def smoothed_cross_entropy(logits, target, smoothing=0.1):\n",
    "    num_classes = logits.size(1)\n",
    "    confidence = 1.0 - smoothing\n",
    "    with torch.no_grad():\n",
    "        true_dist = torch.zeros_like(logits)\n",
    "        true_dist.fill_(smoothing / (num_classes - 1))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), confidence)\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n",
    "\n",
    "# -------------------- Helper --------------------\n",
    "def get_preds_from_logits(logits):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    preds = torch.argmax(probs, dim=-1)\n",
    "    return preds, probs\n",
    "\n",
    "#  Updated train_model_for_level with support for:\n",
    "# - AMP\n",
    "# - Gradient Checkpointing (optional)\n",
    "# - Flexible loss combinations\n",
    "# - Early stopping\n",
    "# - Training history saving\n",
    "\n",
    "def train_model_for_level(\n",
    "    num_labels, train_loader, val_loader, save_path, level_name=\"level\",\n",
    "    y_train_labels=None, loss_type=\"focal+dice+contrastive\", contrastive_weight=0.2,\n",
    "    label_smoothing=0.0, gradient_checkpointing=False, use_amp=True, patience=2\n",
    "):\n",
    "    import json\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    from transformers import AdamW, get_scheduler, DebertaV2ForSequenceClassification\n",
    "    from torch.cuda.amp import GradScaler, autocast\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(OUTPUT_DIR, f\"run_{run_id}\")\n",
    "    os.makedirs(os.path.join(run_dir, \"plots\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(run_dir, \"logs\"), exist_ok=True)\n",
    "\n",
    "    model = DebertaV2ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels).to(device)\n",
    "    if gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    model.config.output_hidden_states = True\n",
    "\n",
    "    if y_train_labels is not None:\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(y_train_labels), y=y_train_labels)\n",
    "    else:\n",
    "        all_train_labels = [label.item() for batch in train_loader for label in batch['labels']]\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(all_train_labels), y=all_train_labels)\n",
    "\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    focal = FocalLoss(alpha=class_weights_tensor, gamma=2.0, label_smoothing=label_smoothing)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=EPOCHS * len(train_loader))\n",
    "    scaler = GradScaler(enabled=use_amp)\n",
    "\n",
    "    best_f1, patience_counter = 0, 0\n",
    "    train_losses, train_accuracies, train_f1s = [], [], []\n",
    "    val_accuracies, val_f1s = [], []\n",
    "    best_metrics = {}\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss, all_preds, all_labels = 0, [], []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"[{level_name}] Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                outputs = model(**batch)\n",
    "                logits = outputs.logits\n",
    "                loss = 0\n",
    "\n",
    "                if \"focal\" in loss_type:\n",
    "                    loss += focal(logits, batch['labels'])\n",
    "                elif label_smoothing > 0:\n",
    "                    loss += smoothed_cross_entropy(logits, batch['labels'], smoothing=label_smoothing)\n",
    "                else:\n",
    "                    loss += F.cross_entropy(logits, batch['labels'], weight=class_weights_tensor)\n",
    "\n",
    "                if \"dice\" in loss_type:\n",
    "                    loss += dice_loss(logits, batch['labels'])\n",
    "                if \"contrastive\" in loss_type:\n",
    "                    hidden_states = outputs.hidden_states[-1][:, 0, :]\n",
    "                    if hidden_states.size(0) > 1:\n",
    "                        loss += contrastive_weight * apply_contrastive_supervision(hidden_states, batch['labels'])\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds, _ = get_preds_from_logits(logits)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        train_losses.append(total_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_f1s.append(train_f1)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds, val_labels, val_probs = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                    outputs = model(**batch)\n",
    "                logits = outputs.logits\n",
    "                preds, probs = get_preds_from_logits(logits)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(batch['labels'].cpu().numpy())\n",
    "                val_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "\n",
    "        try:\n",
    "            val_labels_bin = label_binarize(val_labels, classes=list(range(num_labels)))\n",
    "            roc_auc = roc_auc_score(val_labels_bin, val_probs, average='macro', multi_class='ovr')\n",
    "        except Exception as e:\n",
    "            print(f\" ROC AUC calculation failed: {e}\")\n",
    "            roc_auc = None\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        print(f\" Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\" Train Loss: {total_loss:.4f}\")\n",
    "        print(f\" Train Acc: {train_acc:.4f} |  Train F1: {train_f1:.4f}\")\n",
    "        print(f\" Val Acc:   {val_acc:.4f} |  Val F1:   {val_f1:.4f}\")\n",
    "        if roc_auc is not None:\n",
    "            print(f\" ROC AUC:   {roc_auc:.4f}\")\n",
    "        print(\"=\" * 80, flush=True)\n",
    "\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            precision, recall, f1_metric, _ = precision_recall_fscore_support(val_labels, val_preds, average='weighted')\n",
    "            best_metrics = {\n",
    "                \"val_precision_weighted\": precision,\n",
    "                \"val_recall_weighted\": recall,\n",
    "                \"val_f1_weighted\": f1_metric,\n",
    "                \"roc_auc\": roc_auc\n",
    "            }\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\" Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # Plot Training Curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(train_f1s, label=\"Train F1\")\n",
    "    plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(val_f1s, label=\"Val F1\")\n",
    "    plt.plot(val_accuracies, label=\"Val Accuracy\")\n",
    "    plt.legend(), plt.grid(True)\n",
    "    plt.title(f\" Training Curve - {level_name}\")\n",
    "    plt.savefig(f\"{run_dir}/plots/loss_f1_curve_{level_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save history log\n",
    "    history = {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"train_f1s\": train_f1s,\n",
    "        \"val_f1s\": val_f1s,\n",
    "        \"val_accuracies\": val_accuracies,\n",
    "        **best_metrics\n",
    "    }\n",
    "    with open(os.path.join(run_dir, \"logs\", f\"history_{level_name}.json\"), \"w\") as f:\n",
    "        json.dump(history, f, indent=4)\n",
    "\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# -------------------- Evaluation --------------------\n",
    "# -------------------- Evaluation --------------------\n",
    "\n",
    "\n",
    "def compute_roc_auc(y_true, y_probs, num_labels):\n",
    "    \"\"\"\n",
    "    Compute macro-averaged ROC AUC for multi-class classification.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y_true_bin = label_binarize(y_true, classes=list(range(num_labels)))\n",
    "        roc_auc = roc_auc_score(y_true_bin, y_probs, average=\"macro\", multi_class=\"ovr\")\n",
    "        return roc_auc\n",
    "    except Exception as e:\n",
    "        print(f\" ROC AUC computation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def evaluate_saved_model(model_path, dataloader, num_labels, class_names=None, return_outputs=False):\n",
    "    \"\"\"\n",
    "    Load a saved model, evaluate it on the provided dataloader, and print metrics.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to .pt checkpoint.\n",
    "        dataloader (DataLoader): Validation/test DataLoader.\n",
    "        num_labels (int): Number of output classes.\n",
    "        class_names (list or None): Label names.\n",
    "        return_outputs (bool): Whether to return predictions, labels, and probabilities.\n",
    "\n",
    "    Returns:\n",
    "        Optional: preds, labels, probs\n",
    "    \"\"\"\n",
    "    model = DebertaV2ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    preds, labels, probs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            pred, prob = get_preds_from_logits(logits)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(batch['labels'].cpu().numpy())\n",
    "            probs.extend(prob.cpu().numpy())\n",
    "\n",
    "    # Print classification report\n",
    "    try:\n",
    "        report = classification_report(labels, preds, target_names=class_names, digits=4)\n",
    "    except:\n",
    "        report = classification_report(labels, preds, digits=4)\n",
    "    print(report)\n",
    "\n",
    "    # Compute ROC AUC\n",
    "    roc_auc = compute_roc_auc(labels, probs, num_labels)\n",
    "    if roc_auc is not None:\n",
    "        print(f\" ROC AUC (macro, OVR): {roc_auc:.4f}\")\n",
    "\n",
    "    if return_outputs:\n",
    "        return preds, labels, probs, roc_auc\n",
    "\n",
    "\n",
    "def load_model_for_inference(num_labels, path, device):\n",
    "    \"\"\"\n",
    "    Load a trained model for inference.\n",
    "    \"\"\"\n",
    "    model = DebertaV2ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def visualize_model_performance(true_labels, pred_labels, class_names, title, save_path):\n",
    "    \"\"\"\n",
    "    Save classification report and plot confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        true_labels (List[int]): Ground truth labels.\n",
    "        pred_labels (List[int]): Model predictions.\n",
    "        class_names (List[str]): Names of the classes.\n",
    "        title (str): Plot title.\n",
    "        save_path (str): File path to save the confusion matrix.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        report = classification_report(true_labels, pred_labels, target_names=class_names, digits=4)\n",
    "    except:\n",
    "        report = classification_report(true_labels, pred_labels, digits=4)\n",
    "\n",
    "    print(report)\n",
    "\n",
    "    # Save classification report to .txt\n",
    "    with open(save_path.replace(\".png\", \"_report.txt\"), \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(true_labels, pred_labels, class_names, title, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ub5zZaJbnTo"
   },
   "source": [
    "# **LEVEL-2 CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBXSwgcmbl6O"
   },
   "outputs": [],
   "source": [
    "import os, pickle, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from transformers import DebertaV2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# ==================== ENV SETUP ====================\n",
    "SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "VAL_BATCH_SIZE = 32\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/FIRE\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ========== DATASET CLASS & COLLATE ==========\n",
    "class CryptoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"text\": self.texts[idx],\n",
    "            \"label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts = [x[\"text\"] for x in batch]\n",
    "    labels = torch.tensor([x[\"label\"] for x in batch], dtype=torch.long)\n",
    "    tokenizer_output = tokenizer(\n",
    "        texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": tokenizer_output[\"input_ids\"],\n",
    "        \"attention_mask\": tokenizer_output[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# ========== FOCAL + CONTRASTIVE ==========\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.alpha = torch.tensor(alpha).to(device) if alpha is not None else None\n",
    "        self.gamma = gamma\n",
    "        self.smoothing = label_smoothing\n",
    "    def forward(self, logits, target):\n",
    "        ce = torch.nn.functional.cross_entropy(logits, target, reduction='none', label_smoothing=self.smoothing)\n",
    "        pt = torch.exp(-ce)\n",
    "        focal = (1 - pt) ** self.gamma * ce\n",
    "        if self.alpha is not None:\n",
    "            at = self.alpha[target]\n",
    "            focal = at * focal\n",
    "        return focal.mean()\n",
    "\n",
    "def compute_supervised_contrastive_loss(cls_emb, labels, temperature=0.3):\n",
    "    normalized = torch.nn.functional.normalize(cls_emb, dim=1)\n",
    "    similarity_matrix = torch.matmul(normalized, normalized.T) / temperature\n",
    "    labels = labels.contiguous().view(-1, 1)\n",
    "    mask = torch.eq(labels, labels.T).float().to(device)\n",
    "    logits_mask = torch.ones_like(mask) - torch.eye(mask.size(0)).to(device)\n",
    "    mask *= logits_mask\n",
    "    exp_logits = torch.exp(similarity_matrix) * logits_mask\n",
    "    log_prob = similarity_matrix - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)\n",
    "    mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-12)\n",
    "    return -mean_log_prob_pos.mean()\n",
    "\n",
    "# ========== TRAIN FUNCTION ==========\n",
    "def train_model_for_level(\n",
    "    num_labels, train_loader, val_loader, save_path, level_name,\n",
    "    y_train_labels, loss_type=\"focal+dice+contrastive\", contrastive_weight=0.3,\n",
    "    label_smoothing=0.1, use_amp=True, gradient_checkpointing=False,\n",
    "    patience=3, epochs=5\n",
    "):\n",
    "    from transformers import DebertaV2ForSequenceClassification, get_cosine_schedule_with_warmup\n",
    "    from torch.cuda.amp import GradScaler, autocast\n",
    "    model = DebertaV2ForSequenceClassification.from_pretrained(\n",
    "        \"microsoft/deberta-v3-small\", num_labels=num_labels, output_hidden_states=True\n",
    "    ).to(device)\n",
    "    if gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, int(0.1 * total_steps), total_steps)\n",
    "    scaler = GradScaler(enabled=use_amp)\n",
    "\n",
    "    class_counts = np.bincount(y_train_labels)\n",
    "    class_weights = 1.0 / (np.log(1.01 + class_counts))\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    loss_fn = FocalLoss(alpha=class_weights.tolist(), gamma=2.0, label_smoothing=label_smoothing)\n",
    "\n",
    "    best_f1 = -1\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nüöÇ {level_name} Epoch {epoch+1}/{epochs}\")\n",
    "        model.train(); total_loss = 0; preds, targets = [], []\n",
    "        for batch in tqdm(train_loader, desc=f\"[{level_name}] Training\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(enabled=use_amp):\n",
    "                outputs = model(**batch)\n",
    "                logits = outputs.logits\n",
    "                ce_loss = loss_fn(logits, batch[\"labels\"])\n",
    "                if \"contrastive\" in loss_type:\n",
    "                    cls_emb = torch.stack(outputs.hidden_states[-4:], dim=0).mean(0)[:, 0]\n",
    "                    if batch[\"labels\"].unique().numel() > 1:\n",
    "                        con_loss = compute_supervised_contrastive_loss(cls_emb, batch[\"labels\"])\n",
    "                        ce_loss += contrastive_weight * con_loss\n",
    "            scaler.scale(ce_loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            total_loss += ce_loss.item()\n",
    "            preds += logits.argmax(dim=-1).cpu().tolist()\n",
    "            targets += batch[\"labels\"].cpu().tolist()\n",
    "\n",
    "        train_acc = accuracy_score(targets, preds)\n",
    "        train_f1 = f1_score(targets, preds, average=\"weighted\")\n",
    "        print(f\" Train Loss: {total_loss:.4f} | Acc: {train_acc:.4f} | F1: {train_f1:.4f}\")\n",
    "\n",
    "        model.eval(); val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                with autocast(enabled=use_amp):\n",
    "                    logits = model(**batch).logits\n",
    "                val_preds += logits.argmax(dim=-1).cpu().tolist()\n",
    "                val_targets += batch[\"labels\"].cpu().tolist()\n",
    "        val_acc = accuracy_score(val_targets, val_preds)\n",
    "        val_f1 = f1_score(val_targets, val_preds, average=\"weighted\")\n",
    "        print(f\" Val Acc: {val_acc:.4f} | F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\" Saved model: {save_path}\")\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\" Early Stopping\")\n",
    "                break\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    return model\n",
    "\n",
    "\n",
    "# ========== LEVEL 2 EXECUTION ==========\n",
    "if __name__ == \"__main__\":\n",
    "    run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(OUTPUT_DIR, f\"run_{run_id}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    for folder in [\"models\", \"logs\", \"plots\", \"encoders\", \"ensembles\"]:\n",
    "        os.makedirs(os.path.join(run_dir, folder), exist_ok=True)\n",
    "\n",
    "    train_df = pd.read_csv(\"/content/drive/MyDrive/FIRE/crypto_task1_train.csv\")\n",
    "    val_df = pd.read_csv(\"/content/drive/MyDrive/FIRE/crypto_task1_val.csv\")\n",
    "\n",
    "    # --- Fix label encoding ---\n",
    "    label1_map = {0: \"NOISE\", 1: \"OBJECTIVE\", 2: \"SUBJECTIVE\"}\n",
    "    train_df[\"level_1_str\"] = train_df[\"level_1\"].map(label1_map)\n",
    "    val_df[\"level_1_str\"] = val_df[\"level_1\"].map(label1_map)\n",
    "\n",
    "    le1 = LabelEncoder()\n",
    "    train_df[\"level_1_enc\"] = le1.fit_transform(train_df[\"level_1_str\"])\n",
    "    val_df[\"level_1_enc\"] = le1.transform(val_df[\"level_1_str\"])\n",
    "\n",
    "    # SAVE TO FIXED PATH\n",
    "    fixed_level1_encoder_path = \"/content/drive/MyDrive/FIRE/run_20250628_034630/encoders/label_encoder_level_1.pkl\"\n",
    "    pickle.dump(le1, open(fixed_level1_encoder_path, \"wb\"))\n",
    "\n",
    "    tokenizer = DebertaV2Tokenizer.from_pretrained(\"microsoft/deberta-v3-small\")\n",
    "\n",
    "    subjective_code = le1.transform([\"SUBJECTIVE\"])[0]\n",
    "    train_l2_df = train_df[train_df[\"level_1_enc\"] == subjective_code].copy()\n",
    "    val_l2_df = val_df[val_df[\"level_1_enc\"] == subjective_code].copy()\n",
    "\n",
    "    le2 = LabelEncoder()\n",
    "    train_l2_df[\"level_2_enc\"] = le2.fit_transform(train_l2_df[\"level_2\"])\n",
    "    val_l2_df[\"level_2_enc\"] = le2.transform(val_l2_df[\"level_2\"])\n",
    "    pickle.dump(le2, open(f\"{run_dir}/encoders/label_encoder_level_2.pkl\", \"wb\"))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    level2_preds, level2_labels = [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_l2_df, train_l2_df[\"level_2_enc\"])):\n",
    "        print(f\"\\n Fold {fold + 1}/5\")\n",
    "        fold_train = train_l2_df.iloc[train_idx].reset_index(drop=True)\n",
    "        fold_val = train_l2_df.iloc[val_idx].reset_index(drop=True)\n",
    "        train_loader = DataLoader(CryptoDataset(fold_train[\"text\"], fold_train[\"level_2_enc\"], tokenizer), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "        val_loader = DataLoader(CryptoDataset(fold_val[\"text\"], fold_val[\"level_2_enc\"], tokenizer), batch_size=VAL_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "        save_path = f\"{run_dir}/models/level2_fold{fold+1}.pth\"\n",
    "\n",
    "        model = train_model_for_level(\n",
    "            num_labels=len(le2.classes_),\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            save_path=save_path,\n",
    "            level_name=f\"level2_fold{fold+1}\",\n",
    "            y_train_labels=fold_train[\"level_2_enc\"].values,\n",
    "            loss_type=\"focal+contrastive\",\n",
    "            label_smoothing=0.1,\n",
    "            epochs=10,\n",
    "            patience=4\n",
    "        )\n",
    "\n",
    "        model.eval(); fold_preds, fold_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                logits = model(**batch).logits\n",
    "                fold_preds += logits.argmax(dim=-1).cpu().tolist()\n",
    "                fold_labels += batch[\"labels\"].cpu().tolist()\n",
    "        level2_preds.append(fold_preds)\n",
    "        level2_labels.append(fold_labels)\n",
    "\n",
    "    from scipy.stats import mode\n",
    "    majority_preds = mode(np.array(level2_preds), axis=0).mode[0]\n",
    "    true_labels = np.array(level2_labels[0])\n",
    "    acc = accuracy_score(true_labels, majority_preds)\n",
    "    f1 = f1_score(true_labels, majority_preds, average=\"weighted\")\n",
    "    print(f\"\\n Level 2 Ensemble Accuracy: {acc:.4f} | F1: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33297,
     "status": "ok",
     "timestamp": 1751191830881,
     "user": {
      "displayName": "Klh edu",
      "userId": "03499339510403364543"
     },
     "user_tz": -330
    },
    "id": "3MlWXkkxwZaN",
    "outputId": "9fd7e388-e173-46d4-8ea1-36603a84daef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Columns in validation CSV: ['text', 'level_1', 'level_2', 'level_3', 'source']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Fold 1 ‚Äî loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Predicting Fold 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:03<00:00,  7.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Fold 2 ‚Äî loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Predicting Fold 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:04<00:00,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Fold 3 ‚Äî loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Predicting Fold 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:04<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Fold 4 ‚Äî loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Predicting Fold 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:04<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Fold 5 ‚Äî loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Predicting Fold 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:04<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Ensemble Accuracy: 0.8190\n",
      "üéØ F1 (Weighted): 0.8001\n",
      "üìè F1 (Macro):    0.6440\n",
      "üìê F1 (Micro):    0.8190\n",
      "üéØ Precision (Macro): 0.7272\n",
      "üìå Recall (Macro):    0.6103\n",
      "\n",
      "üìã Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8363    0.9545    0.8915       637\n",
      "         1.0     0.8049    0.6439    0.7154       205\n",
      "         2.0     0.5405    0.2326    0.3252        86\n",
      "\n",
      "    accuracy                         0.8190       928\n",
      "   macro avg     0.7272    0.6103    0.6440       928\n",
      "weighted avg     0.8020    0.8190    0.8001       928\n",
      "\n",
      "\n",
      "\n",
      "üîé Platform-wise Evaluation:\n",
      "\n",
      "üì¶ Platform: YOUTUBE\n",
      "Accuracy: 0.8254\n",
      "F1 Weighted: 0.8154\n",
      "F1 Macro:    0.6098\n",
      "Platform Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8204    0.9510    0.8809       245\n",
      "         1.0     0.8730    0.6962    0.7746       158\n",
      "         2.0     0.2500    0.1333    0.1739        15\n",
      "\n",
      "    accuracy                         0.8254       418\n",
      "   macro avg     0.6478    0.5935    0.6098       418\n",
      "weighted avg     0.8198    0.8254    0.8154       418\n",
      "\n",
      "\n",
      "üì¶ Platform: REDDIT\n",
      "Accuracy: 0.8338\n",
      "F1 Weighted: 0.8102\n",
      "F1 Macro:    0.5546\n",
      "Platform Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8728    0.9609    0.9147       307\n",
      "         1.0     0.6176    0.4667    0.5316        45\n",
      "         2.0     0.3846    0.1515    0.2174        33\n",
      "\n",
      "    accuracy                         0.8338       385\n",
      "   macro avg     0.6250    0.5264    0.5546       385\n",
      "weighted avg     0.8011    0.8338    0.8102       385\n",
      "\n",
      "\n",
      "üì¶ Platform: TWITTER\n",
      "Accuracy: 0.7520\n",
      "F1 Weighted: 0.7243\n",
      "F1 Macro:    0.5523\n",
      "Platform Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7619    0.9412    0.8421        85\n",
      "         1.0     0.2500    0.5000    0.3333         2\n",
      "         2.0     0.8125    0.3421    0.4815        38\n",
      "\n",
      "    accuracy                         0.7520       125\n",
      "   macro avg     0.6081    0.5944    0.5523       125\n",
      "weighted avg     0.7691    0.7520    0.7243       125\n",
      "\n",
      "\n",
      "‚úÖ All outputs saved to: /content/drive/MyDrive/FIRE/outputs/ensemble_level2_eval\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "from scipy.stats import mode\n",
    "from transformers import DebertaV2ForSequenceClassification, AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "# === CONFIG ===\n",
    "model_paths = [\n",
    "    \"/content/drive/MyDrive/FIRE/run_20250629_090953/models/level2_fold1.pth\",\n",
    "    \"/content/drive/MyDrive/FIRE/run_20250629_090953/models/level2_fold2.pth\",\n",
    "    \"/content/drive/MyDrive/FIRE/run_20250629_090953/models/level2_fold3.pth\",\n",
    "    \"/content/drive/MyDrive/FIRE/run_20250629_090953/models/level2_fold4.pth\",\n",
    "    \"/content/drive/MyDrive/FIRE/run_20250629_090953/models/level2_fold5.pth\",\n",
    "]\n",
    "model_name = \"microsoft/deberta-v3-small\"\n",
    "label_encoder_path = \"/content/drive/MyDrive/FIRE/run_20250629_090953/encoders/label_encoder_level_2.pkl\"\n",
    "val_csv_path = \"/content/drive/MyDrive/FIRE/crypto_task1_val.csv\"\n",
    "save_dir = \"/content/drive/MyDrive/FIRE/outputs/ensemble_level2_eval\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# === Load label encoder ===\n",
    "with open(label_encoder_path, \"rb\") as f:\n",
    "    le2 = pickle.load(f)\n",
    "\n",
    "# === Load and preprocess validation data ===\n",
    "val_df = pd.read_csv(val_csv_path)\n",
    "print(\" Columns in validation CSV:\", val_df.columns.tolist())\n",
    "\n",
    "# Add [SOURCE] token to text\n",
    "val_df['source_token'] = val_df['source'].str.upper().map({\n",
    "    'REDDIT': '[REDDIT]',\n",
    "    'TWITTER': '[TWITTER]',\n",
    "    'YOUTUBE': '[YOUTUBE]'\n",
    "})\n",
    "val_df['text'] = val_df['source_token'] + ' ' + val_df['text']\n",
    "\n",
    "# Only SUBJECTIVE samples are relevant for Level 2\n",
    "subjective_mask = val_df['level_1'] == 2\n",
    "val_df = val_df[subjective_mask].copy()\n",
    "val_df[\"level_2_enc\"] = le2.transform(val_df[\"level_2\"])\n",
    "true_labels = val_df[\"level_2_enc\"].values\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "encodings = tokenizer(\n",
    "    list(val_df['text']),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "labels = torch.tensor(true_labels)\n",
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    encodings['input_ids'], encodings['attention_mask'], labels\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# === Inference ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "all_fold_preds = []\n",
    "\n",
    "for fold, model_path in enumerate(model_paths):\n",
    "    print(f\"\\n Fold {fold+1} ‚Äî loading model\")\n",
    "    model = DebertaV2ForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=len(le2.classes_)\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    fold_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Predicting Fold {fold+1}\"):\n",
    "            input_ids, attention_mask, _ = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            fold_preds.extend(preds)\n",
    "\n",
    "    all_fold_preds.append(np.array(fold_preds))\n",
    "\n",
    "# === Majority Voting ===\n",
    "ensemble_preds = mode(np.array(all_fold_preds), axis=0).mode.squeeze()\n",
    "pred_labels = le2.inverse_transform(ensemble_preds)\n",
    "\n",
    "# Save predictions\n",
    "val_df[\"preds\"] = ensemble_preds\n",
    "val_df[\"pred_labels\"] = pred_labels\n",
    "val_df.to_csv(os.path.join(save_dir, \"level2_val_predictions.csv\"), index=False)\n",
    "\n",
    "# === Overall Metrics ===\n",
    "acc = accuracy_score(true_labels, ensemble_preds)\n",
    "f1_weighted = f1_score(true_labels, ensemble_preds, average='weighted')\n",
    "f1_macro = f1_score(true_labels, ensemble_preds, average='macro')\n",
    "f1_micro = f1_score(true_labels, ensemble_preds, average='micro')\n",
    "prec_macro = precision_score(true_labels, ensemble_preds, average='macro')\n",
    "recall_macro = recall_score(true_labels, ensemble_preds, average='macro')\n",
    "\n",
    "try:\n",
    "    report = classification_report(\n",
    "        true_labels,\n",
    "        ensemble_preds,\n",
    "        labels=list(range(len(le2.classes_))),\n",
    "        target_names=[str(c) for c in le2.classes_],\n",
    "        digits=4\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\" Error generating classification report:\", e)\n",
    "    report = classification_report(true_labels, ensemble_preds, digits=4)\n",
    "\n",
    "metrics_text = f\"\"\"\n",
    "Ensemble Accuracy:        {acc:.4f}\n",
    "F1 Score (Weighted):      {f1_weighted:.4f}\n",
    "F1 Score (Macro):         {f1_macro:.4f}\n",
    "F1 Score (Micro):         {f1_micro:.4f}\n",
    "Precision (Macro):        {prec_macro:.4f}\n",
    "Recall (Macro):           {recall_macro:.4f}\n",
    "\n",
    "Classification Report:\n",
    "{report}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(metrics_text)\n",
    "with open(os.path.join(save_dir, \"metrics.txt\"), \"w\") as f:\n",
    "    f.write(metrics_text)\n",
    "\n",
    "# === Confusion Matrix ===\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm = confusion_matrix(true_labels, ensemble_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Level 2 Ensemble)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# === Platform-wise Evaluation\n",
    "print(\"\\nPlatform-wise Evaluation:\")\n",
    "platforms = [\"youtube\", \"reddit\", \"twitter\"]\n",
    "\n",
    "for platform in platforms:\n",
    "    mask = val_df['source'].str.lower() == platform\n",
    "    y_true = val_df[\"level_2_enc\"].values[mask]\n",
    "    y_pred = val_df[\"preds\"].values[mask]\n",
    "\n",
    "    print(f\"\\nPlatform: {platform.upper()}\")\n",
    "    if len(y_true) == 0:\n",
    "        print(f\"Warning: No samples for {platform.upper()}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Accuracy:     {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"F1 Weighted:  {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"F1 Macro:     {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "\n",
    "    try:\n",
    "        platform_report = classification_report(\n",
    "            y_true, y_pred,\n",
    "            labels=list(range(len(le2.classes_))),\n",
    "            target_names=[str(c) for c in le2.classes_],\n",
    "            digits=4\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Error generating report:\", e)\n",
    "        platform_report = classification_report(y_true, y_pred, digits=4)\n",
    "\n",
    "    print(f\"Platform Report:\\n{platform_report}\")\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ij2bmnyvbP_N"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOhZ5nUwFU63gQNDFHWHMn1",
   "gpuType": "T4",
   "mount_file_id": "1xkWRnJGRgh1-2A2mgEfBKAPDdyqQf704",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b0d2a52dd794e2280032f317eaba9d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7076bf77fdec4d97981f57cd101e2032",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_58b95719dce74cf4bb37337e836c472c",
      "value": "‚Äá2.46M/2.46M‚Äá[00:00&lt;00:00,‚Äá12.7MB/s]"
     }
    },
    "1232078a2d4845dcb432920da5664218": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5efb16758bf94d919ce318805a32c544",
      "max": 52,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_774475469caa4cb59512ab45f04813cf",
      "value": 52
     }
    },
    "27908a38c6ff4884aa33a2eaa7cdb71e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2c7e3c71aa9c4eb5b926f8f6dde61830": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f63e02771722410e878badaf7fb1d24a",
       "IPY_MODEL_93ed2c04503445939d47a5b9319325a4",
       "IPY_MODEL_7e45c21d654a4e54b0dc1a998a8e6832"
      ],
      "layout": "IPY_MODEL_d8c89aeb8e224fdf9b7a370aba832890"
     }
    },
    "34446b23177146c5afeb1efd254066f1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "44b39c07c5744964a314ed77b98b3894": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d86da9084fa54fe694d8d41b78155214",
       "IPY_MODEL_1232078a2d4845dcb432920da5664218",
       "IPY_MODEL_ea05048187ae4a44a4204e4d18221049"
      ],
      "layout": "IPY_MODEL_4b252fbffce84e4aa6b222fe45e27162"
     }
    },
    "4ac885be819645bbbc2003a1fcf7fa02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b252fbffce84e4aa6b222fe45e27162": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58b95719dce74cf4bb37337e836c472c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5efb16758bf94d919ce318805a32c544": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f6def2df2f2419fa56178bbb45b2675": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64e7ef31555846a1bbcea81036ea3284": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6dfbd178ff61410494469a70bf864108": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7076bf77fdec4d97981f57cd101e2032": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7246485adddc452aa3bc81456295d13b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "774475469caa4cb59512ab45f04813cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7e45c21d654a4e54b0dc1a998a8e6832": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_826ae83fb28047a2aae81fa7645dde57",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8d78c961dec34ac59470d8ab74af79a1",
      "value": "‚Äá578/578‚Äá[00:00&lt;00:00,‚Äá10.7kB/s]"
     }
    },
    "826ae83fb28047a2aae81fa7645dde57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c77483b0cb0454c990be1ddce7a2f30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d78c961dec34ac59470d8ab74af79a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "909110ec77d64d1fa567a315c095577c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93ed2c04503445939d47a5b9319325a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c77483b0cb0454c990be1ddce7a2f30",
      "max": 578,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fb0a0b7a65f042778dba4845a9673bc4",
      "value": 578
     }
    },
    "ae49923797374fba96a9bc37da204683": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cf4245154e304e4897d9c23a2da42c52",
       "IPY_MODEL_fa75f857b8df4d07a3cdea400b34ec3a",
       "IPY_MODEL_0b0d2a52dd794e2280032f317eaba9d8"
      ],
      "layout": "IPY_MODEL_4ac885be819645bbbc2003a1fcf7fa02"
     }
    },
    "b123e09adc9f4f1583c2db2761128873": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb9423f3566549d78a189d9d78ca8e52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf4245154e304e4897d9c23a2da42c52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0b2313047bc475c9186dc687093a533",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_27908a38c6ff4884aa33a2eaa7cdb71e",
      "value": "spm.model:‚Äá100%"
     }
    },
    "d0b2313047bc475c9186dc687093a533": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d86da9084fa54fe694d8d41b78155214": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64e7ef31555846a1bbcea81036ea3284",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_bb9423f3566549d78a189d9d78ca8e52",
      "value": "tokenizer_config.json:‚Äá100%"
     }
    },
    "d8c89aeb8e224fdf9b7a370aba832890": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea05048187ae4a44a4204e4d18221049": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f6def2df2f2419fa56178bbb45b2675",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_7246485adddc452aa3bc81456295d13b",
      "value": "‚Äá52.0/52.0‚Äá[00:00&lt;00:00,‚Äá3.08kB/s]"
     }
    },
    "f63e02771722410e878badaf7fb1d24a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b123e09adc9f4f1583c2db2761128873",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_909110ec77d64d1fa567a315c095577c",
      "value": "config.json:‚Äá100%"
     }
    },
    "fa75f857b8df4d07a3cdea400b34ec3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34446b23177146c5afeb1efd254066f1",
      "max": 2464616,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6dfbd178ff61410494469a70bf864108",
      "value": 2464616
     }
    },
    "fb0a0b7a65f042778dba4845a9673bc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
