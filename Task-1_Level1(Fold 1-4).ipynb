{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === 1. Load datasets ===\n",
    "reddit_path = \"/content/drive/MyDrive/FIRE/CRYPTO-REDDIT-OPINION-TRAIN.csv\"\n",
    "twitter_path = \"/content/drive/MyDrive/FIRE/CRYPTO-TWITTER-OPINION-TRAIN.csv\"\n",
    "youtube_path = \"/content/drive/MyDrive/FIRE/CRYPTO-YOUTUBE-OPINION-TRAIN.csv\"\n",
    "\n",
    "df_reddit = pd.read_csv(reddit_path)\n",
    "df_twitter = pd.read_csv(twitter_path)\n",
    "df_youtube = pd.read_csv(youtube_path)\n",
    "\n",
    "# === 2. Normalize column names ===\n",
    "def normalize_columns(df):\n",
    "    df.columns = df.columns.str.lower().str.strip()\n",
    "    return df\n",
    "\n",
    "df_reddit = normalize_columns(df_reddit)\n",
    "df_twitter = normalize_columns(df_twitter)\n",
    "df_youtube = normalize_columns(df_youtube)\n",
    "\n",
    "# === 3. Standardize Reddit ===\n",
    "df_reddit['text'] = (\n",
    "    df_reddit['title'].fillna('') + ' ' +\n",
    "    df_reddit['selftext'].fillna('') + ' ' +\n",
    "    df_reddit['main'].fillna('')\n",
    ").str.strip()\n",
    "\n",
    "df_reddit = df_reddit.rename(columns={\n",
    "    'level 1': 'level_1',\n",
    "    'level 2': 'level_2',\n",
    "    'level 3': 'level_3'\n",
    "})\n",
    "df_reddit['source'] = 'reddit'\n",
    "df_reddit = df_reddit[['text', 'level_1', 'level_2', 'level_3', 'source']]\n",
    "\n",
    "# === 4. Standardize Twitter ===\n",
    "df_twitter = df_twitter.rename(columns={\n",
    "    'tweet': 'text',\n",
    "    'level 1': 'level_1',\n",
    "    'level 2': 'level_2',\n",
    "    'level 3': 'level_3'\n",
    "})\n",
    "df_twitter['source'] = 'twitter'\n",
    "df_twitter = df_twitter[['text', 'level_1', 'level_2', 'level_3', 'source']]\n",
    "\n",
    "# === 5. Standardize YouTube ===\n",
    "df_youtube = df_youtube.rename(columns={\n",
    "    'comment': 'text',\n",
    "    'level 1': 'level_1',\n",
    "    'level 2': 'level_2',\n",
    "    'level 3': 'level_3'\n",
    "})\n",
    "df_youtube['source'] = 'youtube'\n",
    "df_youtube = df_youtube[['text', 'level_1', 'level_2', 'level_3', 'source']]\n",
    "\n",
    "# === 6. Combine all datasets ===\n",
    "df_all = pd.concat([df_reddit, df_twitter, df_youtube], ignore_index=True)\n",
    "\n",
    "# === 7. Filter/clean valid samples ===\n",
    "def preprocess_task1_dataset(df):\n",
    "    def should_keep(row):\n",
    "        try:\n",
    "            lvl1 = int(row.get('level_1'))\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            lvl2 = int(row.get('level_2')) if pd.notna(row.get('level_2')) else None\n",
    "        except:\n",
    "            lvl2 = None\n",
    "\n",
    "        try:\n",
    "            lvl3 = int(row.get('level_3')) if pd.notna(row.get('level_3')) else None\n",
    "        except:\n",
    "            lvl3 = None\n",
    "\n",
    "        if lvl1 in [0, 1]:\n",
    "            return True\n",
    "        if lvl1 == 2:\n",
    "            if lvl2 is None:\n",
    "                return False\n",
    "            if lvl2 == 0:\n",
    "                return lvl3 is not None\n",
    "            if lvl2 in [1, 2]:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    return df[df.apply(should_keep, axis=1)].reset_index(drop=True)\n",
    "\n",
    "df_cleaned = preprocess_task1_dataset(df_all)\n",
    "\n",
    "# === 7.1 Sanitize column types ===\n",
    "for col in ['level_1', 'level_2', 'level_3']:\n",
    "    df_cleaned[col] = pd.to_numeric(df_cleaned[col], errors='coerce')\n",
    "\n",
    "df_cleaned = df_cleaned[df_cleaned['level_1'].notna()].reset_index(drop=True)\n",
    "df_cleaned['level_1'] = df_cleaned['level_1'].astype(int)\n",
    "df_cleaned.loc[df_cleaned['level_2'].notna(), 'level_2'] = df_cleaned.loc[df_cleaned['level_2'].notna(), 'level_2'].astype(int)\n",
    "df_cleaned.loc[df_cleaned['level_3'].notna(), 'level_3'] = df_cleaned.loc[df_cleaned['level_3'].notna(), 'level_3'].astype(int)\n",
    "\n",
    "print(\"\\n✅ Total before cleaning:\", len(df_all))\n",
    "print(\"✅ Total after cleaning:\", len(df_cleaned))\n",
    "\n",
    "# === 8. Hierarchical Stratified Split per source ===\n",
    "def hierarchical_split_by_source(df, test_size=0.1, random_state=42):\n",
    "    train_dfs, val_dfs = [], []\n",
    "    for source in df['source'].unique():\n",
    "        df_src = df[df['source'] == source]\n",
    "\n",
    "        train_src, val_src = train_test_split(\n",
    "            df_src, test_size=test_size, stratify=df_src['level_1'], random_state=random_state\n",
    "        )\n",
    "\n",
    "        for df_sub in [train_src, val_src]:\n",
    "            mask_l2 = df_sub['level_1'] == 2\n",
    "            if df_sub[mask_l2]['level_2'].nunique() > 1:\n",
    "                pass\n",
    "\n",
    "            mask_l3 = (df_sub['level_1'] == 2) & (df_sub['level_2'] == 0)\n",
    "            if df_sub[mask_l3]['level_3'].nunique() > 1:\n",
    "                pass\n",
    "\n",
    "        train_dfs.append(train_src)\n",
    "        val_dfs.append(val_src)\n",
    "\n",
    "    train_df = pd.concat(train_dfs).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    val_df = pd.concat(val_dfs).sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    return train_df, val_df\n",
    "\n",
    "train_df, val_df = hierarchical_split_by_source(df_cleaned, test_size=0.1)\n",
    "\n",
    "# === 9. Save cleaned + split data ===\n",
    "train_df.to_csv(\"/content/drive/MyDrive/FIRE/crypto_task1_train.csv\", index=False)\n",
    "val_df.to_csv(\"/content/drive/MyDrive/FIRE/crypto_task1_val.csv\", index=False)\n",
    "\n",
    "print(\"\\n✅ Balanced Train/Val CSVs saved.\")\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size:\", len(val_df))\n",
    "print(\"Train source breakdown:\\n\", train_df['source'].value_counts())\n",
    "print(\"Val source breakdown:\\n\", val_df['source'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvpi6adS9fvl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import mode\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import (\n",
    "    DebertaV2Tokenizer,\n",
    "    DebertaV2ForSequenceClassification,\n",
    "    get_scheduler\n",
    ")\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "VAL_BATCH_SIZE = 32\n",
    "MAX_LENGTH = 128\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-5\n",
    "PATIENCE = 2\n",
    "MODEL_NAME = 'microsoft/deberta-v3-small'\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/FIRE/outputs\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "# -------------------- Seed Setup --------------------\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------- Tokenizer --------------------\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"tokenizer\"))\n",
    "\n",
    "# -------------------- Dataset --------------------\n",
    "class CryptoDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "def collate_fn(batch):\n",
    "    keys = batch[0].keys()\n",
    "    return {key: torch.stack([item[key] for item in batch]) for key in keys}\n",
    "\n",
    "\n",
    "# -------------------- Confusion Matrix --------------------\n",
    "def plot_confusion_matrix(labels, preds, classes, title, save_path):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------- Contrastive Supervision --------------------\n",
    "def apply_contrastive_supervision(features, labels, temperature=0.1):\n",
    "    features = F.normalize(features, dim=1)\n",
    "    similarity_matrix = torch.matmul(features, features.T)\n",
    "    labels = labels.contiguous().view(-1, 1)\n",
    "    mask = torch.eq(labels, labels.T).float().to(features.device)\n",
    "\n",
    "    logits = similarity_matrix / temperature\n",
    "    logits_mask = torch.ones_like(mask) - torch.eye(mask.size(0), device=mask.device)\n",
    "    mask = mask * logits_mask\n",
    "\n",
    "    exp_logits = torch.exp(logits) * logits_mask\n",
    "    log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-9)\n",
    "    mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-9)\n",
    "\n",
    "    loss = -mean_log_prob_pos.mean()\n",
    "    return loss\n",
    "\n",
    "# -------------------- Training Losses --------------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, weight=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        pt = torch.exp(logpt)\n",
    "        logpt = (1 - pt) ** self.gamma * logpt\n",
    "        loss = F.nll_loss(logpt, target, weight=self.weight, reduction=self.reduction)\n",
    "        return self.alpha * loss\n",
    "\n",
    "\n",
    "def dice_loss(logits, targets, smooth=1):\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=logits.size(1)).float().to(logits.device)\n",
    "    intersection = (probs * targets_one_hot).sum(dim=0)\n",
    "    cardinality = probs.sum(dim=0) + targets_one_hot.sum(dim=0)\n",
    "    dice = (2. * intersection + smooth) / (cardinality + smooth)\n",
    "    return 1. - dice.mean()\n",
    "\n",
    "\n",
    "def smoothed_cross_entropy(logits, target, smoothing=0.1):\n",
    "    num_classes = logits.size(1)\n",
    "    confidence = 1.0 - smoothing\n",
    "    with torch.no_grad():\n",
    "        true_dist = torch.zeros_like(logits)\n",
    "        true_dist.fill_(smoothing / (num_classes - 1))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), confidence)\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n",
    "\n",
    "\n",
    "# -------------------- Helper --------------------\n",
    "def get_preds_from_logits(logits):\n",
    "    \"\"\"Return predicted class indices and softmax probabilities.\"\"\"\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    preds = torch.argmax(probs, dim=-1)\n",
    "    return preds, probs\n",
    "\n",
    "\n",
    "def train_model_for_level(\n",
    "    num_labels, train_loader, val_loader, save_path, level_name=\"level\",\n",
    "    y_train_labels=None, loss_type=\"focal+dice\", contrastive_weight=0.2, label_smoothing=0.0\n",
    "):\n",
    "    # Ensure plot and log directories exist\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, \"plots\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, \"logs\"), exist_ok=True)\n",
    "\n",
    "    model = DebertaV2ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels).to(device)\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.config.output_hidden_states = True\n",
    "\n",
    "    # Compute class weights\n",
    "    if y_train_labels is not None:\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(y_train_labels), y=y_train_labels)\n",
    "    else:\n",
    "        all_train_labels = [label.item() for batch in train_loader for label in batch['labels']]\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(all_train_labels), y=all_train_labels)\n",
    "\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    focal = FocalLoss(weight=class_weights_tensor)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=EPOCHS * len(train_loader))\n",
    "\n",
    "    best_f1 = 0\n",
    "    patience_counter = 0\n",
    "    train_losses, train_accuracies, train_f1s = [], [], []\n",
    "    val_accuracies, val_f1s = [], []\n",
    "    best_metrics = {}\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"[{level_name}] Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # ===== Loss Calculation =====\n",
    "            loss = 0\n",
    "            if \"focal\" in loss_type:\n",
    "                loss += focal(logits, batch['labels'])\n",
    "            elif label_smoothing > 0:\n",
    "                loss += smoothed_cross_entropy(logits, batch['labels'], smoothing=label_smoothing)\n",
    "            else:\n",
    "                loss += F.cross_entropy(logits, batch['labels'], weight=class_weights_tensor)\n",
    "\n",
    "            if \"dice\" in loss_type:\n",
    "                loss += dice_loss(logits, batch['labels'])\n",
    "\n",
    "            if \"contrastive\" in loss_type:\n",
    "                hidden_states = outputs.hidden_states[-1][:, 0, :]\n",
    "                loss += contrastive_weight * apply_contrastive_supervision(hidden_states, batch['labels'])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds, _ = get_preds_from_logits(logits)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "        train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        train_losses.append(total_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_f1s.append(train_f1)\n",
    "\n",
    "        # ===== Validation =====\n",
    "        model.eval()\n",
    "        val_preds, val_labels, val_probs = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                logits = outputs.logits\n",
    "                preds, probs = get_preds_from_logits(logits)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(batch['labels'].cpu().numpy())\n",
    "                val_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "        try:\n",
    "            val_labels_bin = label_binarize(val_labels, classes=list(range(num_labels)))\n",
    "            roc_auc = roc_auc_score(val_labels_bin, val_probs, average='macro', multi_class='ovr')\n",
    "        except Exception as e:\n",
    "            print(f\" ROC AUC calculation failed: {e}\")\n",
    "            roc_auc = None\n",
    "\n",
    "        print(f\" Epoch {epoch+1} | Loss: {total_loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Train F1: {train_f1:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "\n",
    "            # Save best checkpoint\n",
    "            with open(save_path, \"wb\") as f:\n",
    "                torch.save(model.state_dict(), f)\n",
    "\n",
    "            precision, recall, f1_metric, _ = precision_recall_fscore_support(val_labels, val_preds, average='weighted')\n",
    "            best_metrics = {\n",
    "                \"val_precision_weighted\": precision,\n",
    "                \"val_recall_weighted\": recall,\n",
    "                \"val_f1_weighted\": f1_metric,\n",
    "                \"roc_auc\": roc_auc\n",
    "            }\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(\" Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # ===== Plot Training Curve =====\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(train_f1s, label=\"Train F1\")\n",
    "    plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(val_f1s, label=\"Val F1\")\n",
    "    plt.plot(val_accuracies, label=\"Val Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(f\" Training Curve - {level_name}\")\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/plots/loss_f1_curve_{level_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # ===== Save Training Log =====\n",
    "    history = {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"train_f1s\": train_f1s,\n",
    "        \"val_f1s\": val_f1s,\n",
    "        \"val_accuracies\": val_accuracies,\n",
    "        **best_metrics\n",
    "    }\n",
    "    with open(f\"{OUTPUT_DIR}/logs/history_{level_name}.json\", \"w\") as f:\n",
    "        json.dump(history, f, indent=4)\n",
    "\n",
    "    #  Return the best model (loaded from disk)\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------- Evaluation --------------------\n",
    "def evaluate_saved_model(model_path, dataloader, num_labels):\n",
    "    model = DebertaV2ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            preds.extend(torch.argmax(outputs.logits, dim=-1).cpu().numpy())\n",
    "            labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "    report = classification_report(labels, preds, digits=4)\n",
    "    print(report)\n",
    "\n",
    "def load_model_for_inference(num_labels, path, device):\n",
    "    model = DebertaV2ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def visualize_model_performance(true_labels, pred_labels, class_names, title, save_path):\n",
    "    \"\"\"\n",
    "    Visualizes classification performance using a confusion matrix and saves the classification report.\n",
    "\n",
    "    Args:\n",
    "        true_labels (list or np.array): Ground truth labels.\n",
    "        pred_labels (list or np.array): Predicted labels from the ensemble.\n",
    "        class_names (list): List of class label names (e.g., label_encoder.classes_).\n",
    "        title (str): Title for the plot and report.\n",
    "        save_path (str): Path to save the confusion matrix image.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(true_labels, pred_labels, target_names=class_names, digits=4)\n",
    "    print(report)\n",
    "\n",
    "    # Save report\n",
    "    report_path = save_path.replace(\".png\", \"_report.txt\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(true_labels, pred_labels, class_names, title, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3_9qkuI9rd1"
   },
   "source": [
    "# **LEVEL-1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17O4vWBs9k8L"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import torch\n",
    "from transformers import DebertaV2Tokenizer\n",
    "from scipy.stats import mode\n",
    "\n",
    "# ==== Assumed Pre-defined: SEED, OUTPUT_DIR, BATCH_SIZE, VAL_BATCH_SIZE,\n",
    "# CryptoDataset, collate_fn, train_model_for_level, load_model_for_inference, visualize_model_performance ====\n",
    "\n",
    "LEVEL1_ENCODER_PATH = \"/content/drive/MyDrive/FIRE/outputs/run_20250628_034630/encoders/label_encoder_level_1.pkl\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(OUTPUT_DIR, f\"run_{run_id}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    for subfolder in [\"models\", \"plots\", \"logs\", \"encoders\", \"ensembles\"]:\n",
    "        os.makedirs(os.path.join(run_dir, subfolder), exist_ok=True)\n",
    "\n",
    "    # === Load Data ===\n",
    "    train_df = pd.read_csv(\"/content/drive/MyDrive/FIRE/crypto_task1_train.csv\")\n",
    "    val_df = pd.read_csv(\"/content/drive/MyDrive/FIRE/crypto_task1_val.csv\")\n",
    "\n",
    "    print(f\"\\n Train size: {len(train_df)}, Validation size: {len(val_df)}\")\n",
    "    print(\" Train Source Distribution:\\n\", train_df['source'].value_counts())\n",
    "    print(\" Validation Source Distribution:\\n\", val_df['source'].value_counts())\n",
    "\n",
    "    # === Add [SOURCE] token ===\n",
    "    def add_source_token(df):\n",
    "        df = df.copy()\n",
    "        df['source_token'] = df['source'].str.upper().map({\n",
    "            'REDDIT': '[REDDIT]',\n",
    "            'TWITTER': '[TWITTER]',\n",
    "            'YOUTUBE': '[YOUTUBE]'\n",
    "        })\n",
    "        df['text'] = df['source_token'] + ' ' + df['text']\n",
    "        return df\n",
    "\n",
    "    train_df = add_source_token(train_df)\n",
    "    val_df = add_source_token(val_df)\n",
    "\n",
    "    tokenizer = DebertaV2Tokenizer.from_pretrained(os.path.join(OUTPUT_DIR, \"tokenizer\"))\n",
    "\n",
    "    def save_ensemble_model(preds_list, save_path):\n",
    "        np.save(save_path, np.array(preds_list))\n",
    "\n",
    "    def save_platform_reports(val_sources, platforms, true_labels, majority_preds, label_classes, level_num):\n",
    "        for platform in platforms:\n",
    "            mask = val_sources == platform\n",
    "            platform_true = true_labels[mask]\n",
    "            platform_pred = majority_preds[mask]\n",
    "            report = classification_report(platform_true, platform_pred, target_names=label_classes, digits=4)\n",
    "            print(f\"\\n Level {level_num} - Platform: {platform}\")\n",
    "            print(report)\n",
    "            with open(f\"{run_dir}/logs/level{level_num}_report_{platform}.txt\", \"w\") as f:\n",
    "                f.write(report)\n",
    "\n",
    "    # -------- LEVEL 1 --------\n",
    "    le1 = LabelEncoder()\n",
    "    train_df['level_1_enc'] = le1.fit_transform(train_df['level_1'])\n",
    "    val_df['level_1_enc'] = le1.transform(val_df['level_1'])\n",
    "\n",
    "    # Save label encoder\n",
    "    pickle.dump(le1, open(LEVEL1_ENCODER_PATH, \"wb\"))\n",
    "\n",
    "    skf1 = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf1.split(train_df, train_df['level_1_enc'])):\n",
    "        print(f\"\\n Level 1 Fold {fold + 1}/5\")\n",
    "        train_fold_df = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_fold_df = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        # Weighted Sampling based on 'source'\n",
    "        source_weights = train_fold_df['source'].map({\n",
    "            'YOUTUBE': 2.5,\n",
    "            'REDDIT': 2.5,\n",
    "            'TWITTER': 6.5\n",
    "        }).fillna(1.0).astype(float).values\n",
    "        source_weights = torch.tensor(source_weights, dtype=torch.double)\n",
    "        sampler = WeightedRandomSampler(source_weights, num_samples=len(source_weights), replacement=True)\n",
    "\n",
    "        # Loaders\n",
    "        train_loader = DataLoader(\n",
    "            CryptoDataset(train_fold_df['text'], train_fold_df['level_1_enc'], tokenizer),\n",
    "            batch_size=BATCH_SIZE, sampler=sampler, collate_fn=collate_fn)\n",
    "        val_loader = DataLoader(\n",
    "            CryptoDataset(val_fold_df['text'], val_fold_df['level_1_enc'], tokenizer),\n",
    "            batch_size=VAL_BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "        model_path = os.path.join(run_dir, f\"models/level1_fold{fold + 1}.pth\")\n",
    "\n",
    "        model = train_model_for_level(\n",
    "            num_labels=len(le1.classes_),\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            save_path=model_path,\n",
    "            level_name=f\"level1_fold{fold + 1}\",\n",
    "            y_train_labels=train_fold_df['level_1_enc'],\n",
    "            loss_type=\"focal+dice+contrastive\",\n",
    "            label_smoothing=0.1\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN1dKpG7cLjYQrbd3MQUpsY",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
