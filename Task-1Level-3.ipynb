{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254,
     "referenced_widgets": [
      "9fc47a2f2a184ff5be80bd5f26d29aaa",
      "08d57cf75def4f769b8e3dfdfbd5d185",
      "a90b4e5d1b114d5fa38248d03bce3bb5",
      "fc36432f67324730aaad16eaed80fd3b",
      "2b83c235cbb04254b8a53805ceec4835",
      "6ec7e1d958f448bebf51382d090a72d3",
      "d349dde5e4214e308c4e30902fe26eb9",
      "fa8158d2f0034e88b91112a0d59c39e5",
      "3cf72ffaa4e346f8b116bc6984decd44",
      "df6801a544554ac18106282c0fc44511",
      "97bba8001ec64367ade6f56ec2cfef80",
      "721e119e9c664d1492ff38a7b2ec8752",
      "c3cdc5c057d247ddb2086a129cbb7a84",
      "fdb89591f9be4739ba5ddb26470437d3",
      "f415f2171d814452bd3fa9c3bab8b70d",
      "026946106a0a4898a4ed27af1f90972a",
      "d83ca0d931b2453eb53dcfabff88fcd8",
      "8dd65ceaca524e38b96e43f29b25930d",
      "935630bf0cef48418db6b4b28fb32475",
      "cd4f66ca38504c1db54d327b501f3b2a",
      "7ff248e227534f1fb063383982171076",
      "09a723d805f9480f8e3be5f40c599a44",
      "3e4b37007338486f9a1bc42d92e66676",
      "e771d40c267645dcb2033ac56f2bdf25",
      "0c3d13a6100a4156ae0cb7fe2850867d",
      "0c1c2da471444c589ace962b66e92658",
      "9972a70aad4045f6b7fa063f316a5f0e",
      "8760dd9c441b4477be9a56ae2e051964",
      "4d914ae7555b4ddd9748393ce7c1b9f3",
      "a8be996f677e46849e0f0157dd390c66",
      "ac2577a08be24dea9ffbded4a50fe8f9",
      "38a1725cba214785b84ed19c544b1b46",
      "fe0785eeda2c4aaf8b96fa35fd67a7ce"
     ]
    },
    "executionInfo": {
     "elapsed": 45244,
     "status": "ok",
     "timestamp": 1751198989315,
     "user": {
      "displayName": "VT Rushi Kannan",
      "userId": "09227156192590121303"
     },
     "user_tz": -330
    },
    "id": "fHqcSmMicYTI",
    "outputId": "7e3c7744-f897-439e-a47d-c92272704521"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc47a2f2a184ff5be80bd5f26d29aaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721e119e9c664d1492ff38a7b2ec8752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4b37007338486f9a1bc42d92e66676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -------------------- Imports --------------------\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import mode\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import (\n",
    "    DebertaV2Tokenizer,\n",
    "    DebertaV2ForSequenceClassification,\n",
    "    get_scheduler\n",
    ")\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import joblib\n",
    "\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "VAL_BATCH_SIZE = 32\n",
    "MAX_LENGTH = 128\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 4e-5\n",
    "PATIENCE = 1\n",
    "MODEL_NAME = 'microsoft/deberta-v3-small'\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/FIRE/outputs\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------- Seed Setup --------------------\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------- Tokenizer --------------------\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"tokenizer\"))\n",
    "\n",
    "# -------------------- Dataset --------------------\n",
    "class CryptoDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.stack([b['input_ids'] for b in batch]),\n",
    "        'attention_mask': torch.stack([b['attention_mask'] for b in batch]),\n",
    "        'labels': torch.stack([b['labels'] for b in batch]),\n",
    "    }\n",
    "\n",
    "\n",
    "# -------------------- Confusion Matrix --------------------\n",
    "def plot_confusion_matrix(labels, preds, classes, title, save_path):\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------- Contrastive Supervision --------------------\n",
    "def apply_contrastive_supervision(features, labels, temperature=0.1):\n",
    "    features = F.normalize(features, dim=1)\n",
    "    similarity_matrix = torch.matmul(features, features.T)\n",
    "    labels = labels.contiguous().view(-1, 1)\n",
    "    mask = torch.eq(labels, labels.T).float().to(features.device)\n",
    "\n",
    "    logits = similarity_matrix / temperature\n",
    "    logits_mask = torch.ones_like(mask) - torch.eye(mask.size(0), device=mask.device)\n",
    "    mask = mask * logits_mask\n",
    "\n",
    "    exp_logits = torch.exp(logits) * logits_mask\n",
    "    log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-9)\n",
    "    mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-9)\n",
    "\n",
    "    loss = -mean_log_prob_pos.mean()\n",
    "    return loss\n",
    "\n",
    "# -------------------- Training Losses --------------------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, weight=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        pt = torch.exp(logpt)\n",
    "        logpt = (1 - pt) ** self.gamma * logpt\n",
    "        loss = F.nll_loss(logpt, target, weight=self.weight, reduction=self.reduction)\n",
    "        return self.alpha * loss\n",
    "\n",
    "def dice_loss(logits, targets, smooth=1):\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    targets_one_hot = F.one_hot(targets, num_classes=logits.size(1)).float().to(logits.device)\n",
    "    intersection = (probs * targets_one_hot).sum(dim=0)\n",
    "    cardinality = probs.sum(dim=0) + targets_one_hot.sum(dim=0)\n",
    "    dice = (2. * intersection + smooth) / (cardinality + smooth)\n",
    "    return 1. - dice.mean()\n",
    "\n",
    "def smoothed_cross_entropy(logits, target, smoothing=0.1):\n",
    "    num_classes = logits.size(1)\n",
    "    confidence = 1.0 - smoothing\n",
    "    with torch.no_grad():\n",
    "        true_dist = torch.zeros_like(logits)\n",
    "        true_dist.fill_(smoothing / (num_classes - 1))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), confidence)\n",
    "    log_probs = F.log_softmax(logits, dim=1)\n",
    "    return torch.mean(torch.sum(-true_dist * log_probs, dim=1))\n",
    "\n",
    "# -------------------- Helper --------------------\n",
    "def get_preds_from_logits(logits):\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    preds = torch.argmax(probs, dim=-1)\n",
    "    return preds, probs\n",
    "\n",
    "#  Updated train_model_for_level with support for:\n",
    "# - AMP\n",
    "# - Gradient Checkpointing (optional)\n",
    "# - Flexible loss combinations\n",
    "# - Early stopping\n",
    "# - Training history saving\n",
    "\n",
    "def train_model_for_level(\n",
    "    num_labels, train_loader, val_loader, save_path, level_name=\"level\",\n",
    "    y_train_labels=None, loss_type=\"focal+dice+contrastive\", contrastive_weight=0.2,\n",
    "    label_smoothing=0.0, gradient_checkpointing=False, use_amp=True, patience=2\n",
    "):\n",
    "    import json\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    from transformers import AdamW, get_scheduler, DebertaV2ForSequenceClassification\n",
    "    from torch.cuda.amp import GradScaler, autocast\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(OUTPUT_DIR, f\"run_{run_id}\")\n",
    "    os.makedirs(os.path.join(run_dir, \"plots\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(run_dir, \"logs\"), exist_ok=True)\n",
    "\n",
    "    model = DebertaV2ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels).to(device)\n",
    "    if gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    model.config.output_hidden_states = True\n",
    "\n",
    "    if y_train_labels is not None:\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(y_train_labels), y=y_train_labels)\n",
    "    else:\n",
    "        all_train_labels = [label.item() for batch in train_loader for label in batch['labels']]\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(all_train_labels), y=all_train_labels)\n",
    "\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    focal = FocalLoss(alpha=class_weights_tensor, gamma=2.0, label_smoothing=label_smoothing)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=EPOCHS * len(train_loader))\n",
    "    scaler = GradScaler(enabled=use_amp)\n",
    "\n",
    "    best_f1, patience_counter = 0, 0\n",
    "    train_losses, train_accuracies, train_f1s = [], [], []\n",
    "    val_accuracies, val_f1s = [], []\n",
    "    best_metrics = {}\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss, all_preds, all_labels = 0, [], []\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"[{level_name}] Epoch {epoch+1}/{EPOCHS}\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                outputs = model(**batch)\n",
    "                logits = outputs.logits\n",
    "                loss = 0\n",
    "\n",
    "                if \"focal\" in loss_type:\n",
    "                    loss += focal(logits, batch['labels'])\n",
    "                elif label_smoothing > 0:\n",
    "                    loss += smoothed_cross_entropy(logits, batch['labels'], smoothing=label_smoothing)\n",
    "                else:\n",
    "                    loss += F.cross_entropy(logits, batch['labels'], weight=class_weights_tensor)\n",
    "\n",
    "                if \"dice\" in loss_type:\n",
    "                    loss += dice_loss(logits, batch['labels'])\n",
    "                if \"contrastive\" in loss_type:\n",
    "                    hidden_states = outputs.hidden_states[-1][:, 0, :]\n",
    "                    if hidden_states.size(0) > 1:\n",
    "                        loss += contrastive_weight * apply_contrastive_supervision(hidden_states, batch['labels'])\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds, _ = get_preds_from_logits(logits)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch['labels'].cpu().numpy())\n",
    "\n",
    "        train_acc = accuracy_score(all_labels, all_preds)\n",
    "        train_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        train_losses.append(total_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_f1s.append(train_f1)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds, val_labels, val_probs = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                with autocast(device_type=\"cuda\", enabled=use_amp):\n",
    "                    outputs = model(**batch)\n",
    "                logits = outputs.logits\n",
    "                preds, probs = get_preds_from_logits(logits)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels.extend(batch['labels'].cpu().numpy())\n",
    "                val_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "\n",
    "        try:\n",
    "            val_labels_bin = label_binarize(val_labels, classes=list(range(num_labels)))\n",
    "            roc_auc = roc_auc_score(val_labels_bin, val_probs, average='macro', multi_class='ovr')\n",
    "        except Exception as e:\n",
    "            print(f\" ROC AUC calculation failed: {e}\")\n",
    "            roc_auc = None\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        print(f\" Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\" Train Loss: {total_loss:.4f}\")\n",
    "        print(f\" Train Acc: {train_acc:.4f} |  Train F1: {train_f1:.4f}\")\n",
    "        print(f\" Val Acc:   {val_acc:.4f} |  Val F1:   {val_f1:.4f}\")\n",
    "        if roc_auc is not None:\n",
    "            print(f\" ROC AUC:   {roc_auc:.4f}\")\n",
    "        print(\"=\" * 80, flush=True)\n",
    "\n",
    "        val_accuracies.append(val_acc)\n",
    "        val_f1s.append(val_f1)\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            precision, recall, f1_metric, _ = precision_recall_fscore_support(val_labels, val_preds, average='weighted')\n",
    "            best_metrics = {\n",
    "                \"val_precision_weighted\": precision,\n",
    "                \"val_recall_weighted\": recall,\n",
    "                \"val_f1_weighted\": f1_metric,\n",
    "                \"roc_auc\": roc_auc\n",
    "            }\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"‚èπ Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # Plot Training Curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(train_f1s, label=\"Train F1\")\n",
    "    plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(val_f1s, label=\"Val F1\")\n",
    "    plt.plot(val_accuracies, label=\"Val Accuracy\")\n",
    "    plt.legend(), plt.grid(True)\n",
    "    plt.title(f\" Training Curve - {level_name}\")\n",
    "    plt.savefig(f\"{run_dir}/plots/loss_f1_curve_{level_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Save history log\n",
    "    history = {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"train_f1s\": train_f1s,\n",
    "        \"val_f1s\": val_f1s,\n",
    "        \"val_accuracies\": val_accuracies,\n",
    "        **best_metrics\n",
    "    }\n",
    "    with open(os.path.join(run_dir, \"logs\", f\"history_{level_name}.json\"), \"w\") as f:\n",
    "        json.dump(history, f, indent=4)\n",
    "\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# -------------------- Evaluation --------------------\n",
    "# -------------------- Evaluation --------------------\n",
    "\n",
    "\n",
    "def compute_roc_auc(y_true, y_probs, num_labels):\n",
    "    \"\"\"\n",
    "    Compute macro-averaged ROC AUC for multi-class classification.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y_true_bin = label_binarize(y_true, classes=list(range(num_labels)))\n",
    "        roc_auc = roc_auc_score(y_true_bin, y_probs, average=\"macro\", multi_class=\"ovr\")\n",
    "        return roc_auc\n",
    "    except Exception as e:\n",
    "        print(f\" ROC AUC computation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def evaluate_saved_model(model_path, dataloader, num_labels, class_names=None, return_outputs=False):\n",
    "    \"\"\"\n",
    "    Load a saved model, evaluate it on the provided dataloader, and print metrics.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to .pt checkpoint.\n",
    "        dataloader (DataLoader): Validation/test DataLoader.\n",
    "        num_labels (int): Number of output classes.\n",
    "        class_names (list or None): Label names.\n",
    "        return_outputs (bool): Whether to return predictions, labels, and probabilities.\n",
    "\n",
    "    Returns:\n",
    "        Optional: preds, labels, probs\n",
    "    \"\"\"\n",
    "    model = DebertaV2ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    preds, labels, probs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            logits = outputs.logits\n",
    "            pred, prob = get_preds_from_logits(logits)\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "            labels.extend(batch['labels'].cpu().numpy())\n",
    "            probs.extend(prob.cpu().numpy())\n",
    "\n",
    "    # Print classification report\n",
    "    try:\n",
    "        report = classification_report(labels, preds, target_names=class_names, digits=4)\n",
    "    except:\n",
    "        report = classification_report(labels, preds, digits=4)\n",
    "    print(report)\n",
    "\n",
    "    # Compute ROC AUC\n",
    "    roc_auc = compute_roc_auc(labels, probs, num_labels)\n",
    "    if roc_auc is not None:\n",
    "        print(f\" ROC AUC (macro, OVR): {roc_auc:.4f}\")\n",
    "\n",
    "    if return_outputs:\n",
    "        return preds, labels, probs, roc_auc\n",
    "\n",
    "\n",
    "def load_model_for_inference(num_labels, path, device):\n",
    "    \"\"\"\n",
    "    Load a trained model for inference.\n",
    "    \"\"\"\n",
    "    model = DebertaV2ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def visualize_model_performance(true_labels, pred_labels, class_names, title, save_path):\n",
    "    \"\"\"\n",
    "    Save classification report and plot confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        true_labels (List[int]): Ground truth labels.\n",
    "        pred_labels (List[int]): Model predictions.\n",
    "        class_names (List[str]): Names of the classes.\n",
    "        title (str): Plot title.\n",
    "        save_path (str): File path to save the confusion matrix.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        report = classification_report(true_labels, pred_labels, target_names=class_names, digits=4)\n",
    "    except:\n",
    "        report = classification_report(true_labels, pred_labels, digits=4)\n",
    "\n",
    "    print(report)\n",
    "\n",
    "    # Save classification report to .txt\n",
    "    with open(save_path.replace(\".png\", \"_report.txt\"), \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(true_labels, pred_labels, class_names, title, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2137607,
     "status": "error",
     "timestamp": 1751201278100,
     "user": {
      "displayName": "VT Rushi Kannan",
      "userId": "09227156192590121303"
     },
     "user_tz": -330
    },
    "id": "ZpkEVWk5g2od",
    "outputId": "118357d5-5ef4-492b-8cd7-5c55411f8905"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Unique values in level_1 (train): [2 0 1]\n",
      "üîç Unique values in level_2 (train): [ 2.  0. nan  1.]\n",
      "\n",
      "üîÅ Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-3-692409783.py:95: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n",
      "[level3_fold1] Epoch 1:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold1] Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:51<00:00,  5.72it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 1: Train Loss = 270.5017 | Train Acc = 0.4150 | Train F1 = 0.3758 | Val Acc = 0.5444 | Val F1 = 0.3839\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold1] Epoch 2:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold1] Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:43<00:00,  6.77it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 2: Train Loss = 265.6484 | Train Acc = 0.5640 | Train F1 = 0.4558 | Val Acc = 0.7222 | Val F1 = 0.6716\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold1] Epoch 3:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold1] Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:49<00:00,  5.91it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 3: Train Loss = 240.0111 | Train Acc = 0.7496 | Train F1 = 0.6986 | Val Acc = 0.7803 | Val F1 = 0.7306\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold1] Epoch 4:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold1] Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:45<00:00,  6.43it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 4: Train Loss = 224.9056 | Train Acc = 0.7830 | Train F1 = 0.7322 | Val Acc = 0.7889 | Val F1 = 0.7372\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold1] Epoch 5:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold1] Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:58<00:00,  4.98it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 5: Train Loss = 219.5839 | Train Acc = 0.7971 | Train F1 = 0.7506 | Val Acc = 0.7974 | Val F1 = 0.7447\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold1] Epoch 6:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold1] Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:46<00:00,  6.37it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 6: Train Loss = 216.8281 | Train Acc = 0.8110 | Train F1 = 0.7728 | Val Acc = 0.7983 | Val F1 = 0.7557\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold1] Epoch 7:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold1] Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:47<00:00,  6.11it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 7: Train Loss = 213.0904 | Train Acc = 0.8238 | Train F1 = 0.7929 | Val Acc = 0.8043 | Val F1 = 0.7640\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold1] Epoch 8:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold1] Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:46<00:00,  6.30it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 8: Train Loss = 214.5947 | Train Acc = 0.8249 | Train F1 = 0.7943 | Val Acc = 0.8060 | Val F1 = 0.7708\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold1.pth\n",
      "\n",
      "üîÅ Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-3-692409783.py:95: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n",
      "[level3_fold2] Epoch 1:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold2] Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:55<00:00,  5.24it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 1: Train Loss = 271.9362 | Train Acc = 0.5020 | Train F1 = 0.3799 | Val Acc = 0.5436 | Val F1 = 0.3829\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold2] Epoch 2:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold2] Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:46<00:00,  6.25it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 2: Train Loss = 267.1633 | Train Acc = 0.5407 | Train F1 = 0.3902 | Val Acc = 0.5436 | Val F1 = 0.3829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold2] Epoch 3:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold2] Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:45<00:00,  6.42it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 3: Train Loss = 252.0105 | Train Acc = 0.6588 | Train F1 = 0.5958 | Val Acc = 0.7615 | Val F1 = 0.7095\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold2] Epoch 4:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold2] Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:45<00:00,  6.42it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 4: Train Loss = 227.9157 | Train Acc = 0.7734 | Train F1 = 0.7231 | Val Acc = 0.7812 | Val F1 = 0.7298\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold2] Epoch 5:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold2] Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:45<00:00,  6.42it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 5: Train Loss = 222.3213 | Train Acc = 0.7875 | Train F1 = 0.7431 | Val Acc = 0.7923 | Val F1 = 0.7410\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold2] Epoch 6:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold2] Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:45<00:00,  6.47it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 6: Train Loss = 219.5630 | Train Acc = 0.8014 | Train F1 = 0.7670 | Val Acc = 0.7932 | Val F1 = 0.7442\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold2] Epoch 7:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold2] Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:46<00:00,  6.28it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 7: Train Loss = 217.5133 | Train Acc = 0.8097 | Train F1 = 0.7791 | Val Acc = 0.7863 | Val F1 = 0.7429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold2] Epoch 8:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold2] Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:42<00:00,  6.86it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 8: Train Loss = 214.3938 | Train Acc = 0.8123 | Train F1 = 0.7866 | Val Acc = 0.7863 | Val F1 = 0.7428\n",
      "\n",
      "üîÅ Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-3-692409783.py:95: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n",
      "[level3_fold3] Epoch 1:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold3] Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:42<00:00,  6.84it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 1: Train Loss = 274.1547 | Train Acc = 0.3957 | Train F1 = 0.3540 | Val Acc = 0.5441 | Val F1 = 0.3834\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold3] Epoch 2:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold3] Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:43<00:00,  6.74it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 2: Train Loss = 267.1848 | Train Acc = 0.5481 | Train F1 = 0.3996 | Val Acc = 0.6039 | Val F1 = 0.5083\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold3] Epoch 3:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold3] Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:45<00:00,  6.40it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 3: Train Loss = 240.6250 | Train Acc = 0.7435 | Train F1 = 0.6926 | Val Acc = 0.7725 | Val F1 = 0.7215\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold3] Epoch 4:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold3] Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:45<00:00,  6.39it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 4: Train Loss = 226.7806 | Train Acc = 0.7822 | Train F1 = 0.7309 | Val Acc = 0.7810 | Val F1 = 0.7295\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold3] Epoch 5:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold3] Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:45<00:00,  6.41it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 5: Train Loss = 223.6059 | Train Acc = 0.7903 | Train F1 = 0.7380 | Val Acc = 0.7870 | Val F1 = 0.7337\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold3] Epoch 6:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold3] Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:46<00:00,  6.34it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 6: Train Loss = 219.9893 | Train Acc = 0.7963 | Train F1 = 0.7479 | Val Acc = 0.7930 | Val F1 = 0.7406\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold3] Epoch 7:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold3] Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:45<00:00,  6.43it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 7: Train Loss = 216.3455 | Train Acc = 0.8063 | Train F1 = 0.7587 | Val Acc = 0.7947 | Val F1 = 0.7428\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold3.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold3] Epoch 8:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold3] Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:45<00:00,  6.47it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 8: Train Loss = 215.3091 | Train Acc = 0.8087 | Train F1 = 0.7635 | Val Acc = 0.7930 | Val F1 = 0.7399\n",
      "\n",
      "üîÅ Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-3-692409783.py:95: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n",
      "[level3_fold4] Epoch 1:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold4] Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:42<00:00,  6.86it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 1: Train Loss = 271.6054 | Train Acc = 0.4528 | Train F1 = 0.4219 | Val Acc = 0.5449 | Val F1 = 0.3844\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold4] Epoch 2:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold4] Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:43<00:00,  6.74it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 2: Train Loss = 268.1300 | Train Acc = 0.5442 | Train F1 = 0.3958 | Val Acc = 0.5449 | Val F1 = 0.3844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold4] Epoch 3:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold4] Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:42<00:00,  6.84it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 3: Train Loss = 266.2803 | Train Acc = 0.5622 | Train F1 = 0.4598 | Val Acc = 0.6595 | Val F1 = 0.5948\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold4] Epoch 4:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold4] Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:47<00:00,  6.16it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 4: Train Loss = 242.2841 | Train Acc = 0.7204 | Train F1 = 0.6713 | Val Acc = 0.7784 | Val F1 = 0.7289\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold4] Epoch 5:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold4] Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:46<00:00,  6.34it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 5: Train Loss = 226.7863 | Train Acc = 0.7700 | Train F1 = 0.7265 | Val Acc = 0.7887 | Val F1 = 0.7351\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r[level3_fold4] Epoch 6:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold4] Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:45<00:00,  6.38it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 6: Train Loss = 221.0464 | Train Acc = 0.7926 | Train F1 = 0.7539 | Val Acc = 0.7913 | Val F1 = 0.7427\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold4] Epoch 7:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold4] Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:46<00:00,  6.31it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 7: Train Loss = 218.9074 | Train Acc = 0.7950 | Train F1 = 0.7571 | Val Acc = 0.7956 | Val F1 = 0.7553\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold4] Epoch 8:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold4] Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:46<00:00,  6.31it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 8: Train Loss = 217.8447 | Train Acc = 0.8021 | Train F1 = 0.7697 | Val Acc = 0.7947 | Val F1 = 0.7550\n",
      "\n",
      "üîÅ Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-3-692409783.py:95: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=use_amp)\n",
      "[level3_fold5] Epoch 1:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold5] Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:42<00:00,  6.81it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 1: Train Loss = 271.6637 | Train Acc = 0.4590 | Train F1 = 0.4282 | Val Acc = 0.5449 | Val F1 = 0.3844\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold5] Epoch 2:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold5] Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:43<00:00,  6.70it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 2: Train Loss = 266.6303 | Train Acc = 0.5552 | Train F1 = 0.4313 | Val Acc = 0.6048 | Val F1 = 0.5280\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold5] Epoch 3:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold5] Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:45<00:00,  6.43it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 3: Train Loss = 238.7289 | Train Acc = 0.7347 | Train F1 = 0.6902 | Val Acc = 0.7759 | Val F1 = 0.7238\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold5] Epoch 4:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold5] Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:46<00:00,  6.37it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 4: Train Loss = 225.6900 | Train Acc = 0.7792 | Train F1 = 0.7421 | Val Acc = 0.7827 | Val F1 = 0.7293\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold5] Epoch 5:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold5] Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:45<00:00,  6.37it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 5: Train Loss = 219.2373 | Train Acc = 0.8046 | Train F1 = 0.7760 | Val Acc = 0.8067 | Val F1 = 0.7838\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold5] Epoch 6:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold5] Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:46<00:00,  6.33it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 6: Train Loss = 214.7002 | Train Acc = 0.8275 | Train F1 = 0.8109 | Val Acc = 0.8135 | Val F1 = 0.7941\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold5] Epoch 7:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold5] Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:45<00:00,  6.44it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 7: Train Loss = 211.0344 | Train Acc = 0.8369 | Train F1 = 0.8227 | Val Acc = 0.8221 | Val F1 = 0.8112\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[level3_fold5] Epoch 8:   0%|          | 0/293 [00:00<?, ?it/s]/tmp/ipython-input-3-692409783.py:111: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n",
      "[level3_fold5] Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [00:45<00:00,  6.41it/s]\n",
      "/tmp/ipython-input-3-692409783.py:137: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Epoch 8: Train Loss = 211.2309 | Train Acc = 0.8452 | Train F1 = 0.8358 | Val Acc = 0.8221 | Val F1 = 0.8131\n",
      "üíæ Model saved at: /content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold5.pth\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3-692409783.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mlevel3_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mpred_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel3_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0mmajority\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mtrue_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel3_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import os, pickle, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2ForSequenceClassification, get_cosine_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "SEED = 42\n",
    "BATCH_SIZE = 16\n",
    "VAL_BATCH_SIZE = 32\n",
    "MAX_LEN = 128\n",
    "EPOCHS = 8\n",
    "PATIENCE = 3\n",
    "LR = 2e-5\n",
    "MODEL_NAME = \"microsoft/deberta-v3-small\"\n",
    "BASE_DIR = \"/content/drive/MyDrive/FIRE\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -------------------- UTILS --------------------\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed()\n",
    "\n",
    "# -------------------- DATASET --------------------\n",
    "class CryptoDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"text\": self.texts[idx],\n",
    "            \"label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts = [x[\"text\"] for x in batch]\n",
    "    labels = torch.tensor([x[\"label\"] for x in batch], dtype=torch.long)\n",
    "    encoding = tokenizer(texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    return {\n",
    "        \"input_ids\": encoding[\"input_ids\"],\n",
    "        \"attention_mask\": encoding[\"attention_mask\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# -------------------- LOSSES --------------------\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.alpha = torch.tensor(alpha).to(device) if alpha else None\n",
    "        self.gamma = gamma\n",
    "        self.smoothing = label_smoothing\n",
    "    def forward(self, logits, target):\n",
    "        ce = torch.nn.functional.cross_entropy(logits, target, reduction='none', label_smoothing=self.smoothing)\n",
    "        pt = torch.exp(-ce)\n",
    "        focal = (1 - pt) ** self.gamma * ce\n",
    "        if self.alpha is not None:\n",
    "            focal = self.alpha[target] * focal\n",
    "        return focal.mean()\n",
    "\n",
    "def supervised_contrastive_loss(embeddings, labels, temperature=0.3):\n",
    "    norm = torch.nn.functional.normalize(embeddings, dim=1)\n",
    "    sim = torch.matmul(norm, norm.T) / temperature\n",
    "    labels = labels.view(-1, 1)\n",
    "    mask = torch.eq(labels, labels.T).float().to(device)\n",
    "    logits_mask = 1 - torch.eye(mask.size(0)).to(device)\n",
    "    mask *= logits_mask\n",
    "    exp_logits = torch.exp(sim) * logits_mask\n",
    "    log_prob = sim - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)\n",
    "    return -(mask * log_prob).sum(1).div(mask.sum(1) + 1e-12).mean()\n",
    "\n",
    "# -------------------- TRAIN FUNCTION --------------------\n",
    "def train_level3_model(\n",
    "    train_loader, val_loader, save_path, y_train_labels,\n",
    "    num_labels=4, contrastive_weight=0.3, level_name=\"level3_fold\", use_amp=True\n",
    "):\n",
    "    model = DebertaV2ForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels, output_hidden_states=True).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, 0.1 * total_steps, total_steps)\n",
    "    scaler = GradScaler(enabled=use_amp)\n",
    "\n",
    "    class_counts = np.bincount(y_train_labels)\n",
    "    class_weights = 1.0 / (np.log(1.01 + class_counts))\n",
    "    class_weights = class_weights / class_weights.sum()\n",
    "    focal_loss = FocalLoss(alpha=class_weights.tolist(), gamma=2.0, label_smoothing=0.1)\n",
    "\n",
    "    best_f1, patience_counter = -1, 0\n",
    "    train_loss_hist, val_f1_hist, train_f1_hist, train_acc_hist, val_acc_hist = [], [], [], [], []\n",
    "    log_file_path = save_path.replace(\".pth\", \"_log.txt\")\n",
    "    with open(log_file_path, \"w\") as log_file:\n",
    "        for epoch in range(EPOCHS):\n",
    "            model.train(); total_loss = 0; preds, targets = [], []\n",
    "            for batch in tqdm(train_loader, desc=f\"[{level_name}] Epoch {epoch+1}\"):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                optimizer.zero_grad()\n",
    "                with autocast(enabled=use_amp):\n",
    "                    out = model(**batch)\n",
    "                    logits = out.logits\n",
    "                    cls_emb = out.hidden_states[-1][:, 0]\n",
    "                    loss = focal_loss(logits, batch[\"labels\"])\n",
    "                    if batch[\"labels\"].unique().numel() > 1:\n",
    "                        loss += contrastive_weight * supervised_contrastive_loss(cls_emb, batch[\"labels\"])\n",
    "                scaler.scale(loss).backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                total_loss += loss.item()\n",
    "                preds += logits.argmax(dim=-1).cpu().tolist()\n",
    "                targets += batch[\"labels\"].cpu().tolist()\n",
    "\n",
    "            train_loss_hist.append(total_loss)\n",
    "            train_acc = accuracy_score(targets, preds)\n",
    "            train_f1 = f1_score(targets, preds, average=\"weighted\")\n",
    "            train_f1_hist.append(train_f1)\n",
    "            train_acc_hist.append(train_acc)\n",
    "\n",
    "            model.eval(); val_preds, val_targets = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                    with autocast(enabled=use_amp):\n",
    "                        logits = model(**batch).logits\n",
    "                    val_preds += logits.argmax(dim=-1).cpu().tolist()\n",
    "                    val_targets += batch[\"labels\"].cpu().tolist()\n",
    "            val_f1 = f1_score(val_targets, val_preds, average=\"weighted\")\n",
    "            val_acc = accuracy_score(val_targets, val_preds)\n",
    "            val_f1_hist.append(val_f1)\n",
    "            val_acc_hist.append(val_acc)\n",
    "\n",
    "            line = f\" Epoch {epoch+1}: Train Loss = {total_loss:.4f} | Train Acc = {train_acc:.4f} | Train F1 = {train_f1:.4f} | Val Acc = {val_acc:.4f} | Val F1 = {val_f1:.4f}\"\n",
    "            print(line)\n",
    "            log_file.write(line + \"\\n\")\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\" Model saved at: {save_path}\")\n",
    "                log_file.write(f\"Saved model: {save_path}\\n\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= PATIENCE:\n",
    "                    print(\" Early stopping\")\n",
    "                    log_file.write(\"Early stopping\\n\")\n",
    "                    break\n",
    "\n",
    "    # Plot & log\n",
    "    plot_path = save_path.replace(\".pth\", \"_plot.png\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_loss_hist, label=\"Train Loss\")\n",
    "    plt.plot(train_f1_hist, label=\"Train F1\")\n",
    "    plt.plot(train_acc_hist, label=\"Train Acc\")\n",
    "    plt.plot(val_f1_hist, label=\"Val F1\")\n",
    "    plt.plot(val_acc_hist, label=\"Val Acc\")\n",
    "    plt.title(f\"{level_name} Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend(), plt.grid(True)\n",
    "    plt.savefig(plot_path); plt.close()\n",
    "    return model\n",
    "\n",
    "# -------------------- MAIN SCRIPT --------------------\n",
    "if __name__ == \"__main__\":\n",
    "    run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = os.path.join(BASE_DIR, f\"run_{run_id}\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    for sub in [\"models\", \"logs\", \"plots\", \"encoders\"]: os.makedirs(os.path.join(run_dir, sub), exist_ok=True)\n",
    "\n",
    "    train_df = pd.read_csv(f\"{BASE_DIR}/crypto_task1_train.csv\")\n",
    "    val_df = pd.read_csv(f\"{BASE_DIR}/crypto_task1_val.csv\")\n",
    "\n",
    "    print(\" Unique values in level_1 (train):\", train_df[\"level_1\"].unique())\n",
    "    print(\" Unique values in level_2 (train):\", train_df[\"level_2\"].unique())\n",
    "\n",
    "    train_l3_df = train_df[(train_df[\"level_1\"] == 2) & (train_df[\"level_2\"] == 0)].copy().reset_index(drop=True)\n",
    "    val_l3_df = val_df[(val_df[\"level_1\"] == 2) & (val_df[\"level_2\"] == 0)].copy().reset_index(drop=True)\n",
    "\n",
    "    if train_l3_df.empty:\n",
    "        print(\" No NEUTRAL samples found in level_2 under SUBJECTIVE. Skipping Level 3 training.\")\n",
    "    else:\n",
    "        le3 = LabelEncoder()\n",
    "        train_l3_df[\"level_3_enc\"] = le3.fit_transform(train_l3_df[\"level_3\"])\n",
    "        val_l3_df[\"level_3_enc\"] = le3.transform(val_l3_df[\"level_3\"])\n",
    "        pickle.dump(le3, open(f\"{run_dir}/encoders/label_encoder_level_3.pkl\", \"wb\"))\n",
    "\n",
    "        tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "        level3_preds, level3_labels = [], []\n",
    "\n",
    "        for fold, (tr_idx, va_idx) in enumerate(skf.split(train_l3_df, train_l3_df[\"level_3_enc\"])):\n",
    "            print(f\"\\n Fold {fold+1}/5\")\n",
    "            tr_df = train_l3_df.loc[tr_idx].reset_index(drop=True)\n",
    "            va_df = train_l3_df.loc[va_idx].reset_index(drop=True)\n",
    "            tr_loader = DataLoader(CryptoDataset(tr_df[\"text\"], tr_df[\"level_3_enc\"], tokenizer), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "            va_loader = DataLoader(CryptoDataset(va_df[\"text\"], va_df[\"level_3_enc\"], tokenizer), batch_size=VAL_BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "            save_path = os.path.join(run_dir, \"models\", f\"level3_fold{fold+1}.pth\")\n",
    "\n",
    "            model = train_level3_model(\n",
    "                tr_loader, va_loader, save_path,\n",
    "                y_train_labels=tr_df[\"level_3_enc\"].values,\n",
    "                level_name=f\"level3_fold{fold+1}\"\n",
    "            )\n",
    "\n",
    "            model.eval(); preds, targets = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in va_loader:\n",
    "                    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                    logits = model(**batch).logits\n",
    "                    preds += logits.argmax(dim=-1).cpu().tolist()\n",
    "                    targets += batch[\"labels\"].cpu().tolist()\n",
    "            level3_preds.append(preds)\n",
    "            level3_labels.append(targets)\n",
    "\n",
    "        pred_matrix = np.array(level3_preds)\n",
    "        majority = mode(pred_matrix, axis=0).mode[0]\n",
    "        true_labels = np.array(level3_labels[0])\n",
    "        acc = accuracy_score(true_labels, majority)\n",
    "        f1 = f1_score(true_labels, majority, average=\"weighted\")\n",
    "        print(f\"\\n Level 3 Ensemble Accuracy: {acc:.4f} | F1: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33412,
     "status": "ok",
     "timestamp": 1751201376867,
     "user": {
      "displayName": "VT Rushi Kannan",
      "userId": "09227156192590121303"
     },
     "user_tz": -330
    },
    "id": "Aebk4Lz9o1AD",
    "outputId": "fdabedff-bf38-4cb4-bacb-3a63b4c4eca3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Columns in validation CSV: ['text', 'level_1', 'level_2', 'level_3', 'source']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Fold 1 ‚Äî loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Predicting Fold 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Fold 2 ‚Äî loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Predicting Fold 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Fold 3 ‚Äî loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Predicting Fold 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  7.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Fold 4 ‚Äî loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Predicting Fold 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  7.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Fold 5 ‚Äî loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Predicting Fold 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  7.40it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Ensemble Accuracy: 0.7598\n",
      "üéØ F1 (Weighted): 0.7099\n",
      "üìè F1 (Macro):    0.4122\n",
      "üìê F1 (Micro):    0.7598\n",
      "üéØ Precision (Macro): 0.6279\n",
      "üìå Recall (Macro):    0.4531\n",
      "\n",
      "üìã Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5916    0.9200    0.7202       200\n",
      "         1.0     0.9200    0.8768    0.8979       341\n",
      "         2.0     1.0000    0.0156    0.0308        64\n",
      "         3.0     0.0000    0.0000    0.0000        32\n",
      "\n",
      "    accuracy                         0.7598       637\n",
      "   macro avg     0.6279    0.4531    0.4122       637\n",
      "weighted avg     0.7787    0.7598    0.7099       637\n",
      "\n",
      "\n",
      "\n",
      "üîé Platform-wise Evaluation:\n",
      "\n",
      "üì¶ Platform: YOUTUBE\n",
      "Accuracy: 0.8898\n",
      "F1 Weighted: 0.8829\n",
      "F1 Macro:    0.5951\n",
      "Platform Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8291    1.0000    0.9066       131\n",
      "         1.0     1.0000    0.7838    0.8788       111\n",
      "         2.0     0.0000    0.0000    0.0000         0\n",
      "         3.0     0.0000    0.0000    0.0000         3\n",
      "\n",
      "    accuracy                         0.8898       245\n",
      "   macro avg     0.4573    0.4459    0.4463       245\n",
      "weighted avg     0.8964    0.8898    0.8829       245\n",
      "\n",
      "\n",
      "üì¶ Platform: REDDIT\n",
      "Accuracy: 0.8013\n",
      "F1 Weighted: 0.7563\n",
      "F1 Macro:    0.3836\n",
      "Platform Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.5263    0.7273    0.6107        55\n",
      "         1.0     0.8918    0.9581    0.9238       215\n",
      "         2.0     0.0000    0.0000    0.0000         9\n",
      "         3.0     0.0000    0.0000    0.0000        28\n",
      "\n",
      "    accuracy                         0.8013       307\n",
      "   macro avg     0.3545    0.4214    0.3836       307\n",
      "weighted avg     0.7188    0.8013    0.7563       307\n",
      "\n",
      "\n",
      "üì¶ Platform: TWITTER\n",
      "Accuracy: 0.2353\n",
      "F1 Weighted: 0.1664\n",
      "F1 Macro:    0.2167\n",
      "Platform Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.1688    0.9286    0.2857        14\n",
      "         1.0     0.8571    0.4000    0.5455        15\n",
      "         2.0     1.0000    0.0182    0.0357        55\n",
      "         3.0     0.0000    0.0000    0.0000         1\n",
      "\n",
      "    accuracy                         0.2353        85\n",
      "   macro avg     0.5065    0.3367    0.2167        85\n",
      "weighted avg     0.8261    0.2353    0.1664        85\n",
      "\n",
      "\n",
      "‚úÖ All outputs saved to: /content/drive/MyDrive/FIRE/outputs/ensemble_level3_eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# === CONFIRMED LEVEL 3 INFERENCE & EVALUATION SCRIPT ===\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "from scipy.stats import mode\n",
    "from transformers import DebertaV2ForSequenceClassification, AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "# === CONFIG ===\n",
    "model_paths = [\n",
    "    \"/content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold1.pth\",\n",
    "    \"/content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold2.pth\",\n",
    "    \"/content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold3.pth\",\n",
    "    \"/content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold4.pth\",\n",
    "    \"/content/drive/MyDrive/FIRE/run_20250629_121221/models/level3_fold5.pth\",\n",
    "]\n",
    "model_name = \"microsoft/deberta-v3-small\"\n",
    "label_encoder_path = \"/content/drive/MyDrive/FIRE/run_20250629_121221/encoders/label_encoder_level_3.pkl\"\n",
    "val_csv_path = \"/content/drive/MyDrive/FIRE/crypto_task1_val.csv\"\n",
    "save_dir = \"/content/drive/MyDrive/FIRE/outputs/ensemble_level3_eval\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# === Load label encoder ===\n",
    "with open(label_encoder_path, \"rb\") as f:\n",
    "    le3 = pickle.load(f)\n",
    "\n",
    "# === Load and preprocess validation data ===\n",
    "val_df = pd.read_csv(val_csv_path)\n",
    "print(\"Columns in validation CSV:\", val_df.columns.tolist())\n",
    "\n",
    "val_df['source_token'] = val_df['source'].str.upper().map({\n",
    "    'REDDIT': '[REDDIT]',\n",
    "    'TWITTER': '[TWITTER]',\n",
    "    'YOUTUBE': '[YOUTUBE]'\n",
    "})\n",
    "val_df['text'] = val_df['source_token'] + ' ' + val_df['text']\n",
    "\n",
    "# Only NEUTRAL under SUBJECTIVE are valid for Level 3\n",
    "val_df = val_df[(val_df['level_1'] == 2) & (val_df['level_2'] == 0)].copy()\n",
    "val_df[\"level_3_enc\"] = le3.transform(val_df[\"level_3\"])\n",
    "true_labels = val_df[\"level_3_enc\"].values\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "encodings = tokenizer(\n",
    "    list(val_df['text']),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "labels = torch.tensor(true_labels)\n",
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    encodings['input_ids'], encodings['attention_mask'], labels\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# === Inference ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "all_fold_preds = []\n",
    "\n",
    "for fold, model_path in enumerate(model_paths):\n",
    "    print(f\"\\nFold {fold+1} ‚Äî loading model\")\n",
    "    model = DebertaV2ForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=len(le3.classes_)\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    fold_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Predicting Fold {fold+1}\"):\n",
    "            input_ids, attention_mask, _ = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "            fold_preds.extend(preds)\n",
    "\n",
    "    all_fold_preds.append(np.array(fold_preds))\n",
    "\n",
    "# === Majority Voting ===\n",
    "ensemble_preds = mode(np.array(all_fold_preds), axis=0).mode.squeeze()\n",
    "pred_labels = le3.inverse_transform(ensemble_preds)\n",
    "\n",
    "# Save predictions\n",
    "val_df[\"preds\"] = ensemble_preds\n",
    "val_df[\"pred_labels\"] = pred_labels\n",
    "val_df.to_csv(os.path.join(save_dir, \"level3_val_predictions.csv\"), index=False)\n",
    "\n",
    "# === Overall Metrics ===\n",
    "acc = accuracy_score(true_labels, ensemble_preds)\n",
    "f1_weighted = f1_score(true_labels, ensemble_preds, average='weighted')\n",
    "f1_macro = f1_score(true_labels, ensemble_preds, average='macro')\n",
    "f1_micro = f1_score(true_labels, ensemble_preds, average='micro')\n",
    "prec_macro = precision_score(true_labels, ensemble_preds, average='macro')\n",
    "recall_macro = recall_score(true_labels, ensemble_preds, average='macro')\n",
    "\n",
    "try:\n",
    "    report = classification_report(\n",
    "        true_labels,\n",
    "        ensemble_preds,\n",
    "        labels=list(range(len(le3.classes_))),\n",
    "        target_names=[str(c) for c in le3.classes_],\n",
    "        digits=4\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Error generating classification report:\", e)\n",
    "    report = classification_report(true_labels, ensemble_preds, digits=4)\n",
    "\n",
    "metrics_text = f\"\"\"\n",
    "Ensemble Accuracy: {acc:.4f}\n",
    "F1 (Weighted): {f1_weighted:.4f}\n",
    "F1 (Macro):    {f1_macro:.4f}\n",
    "F1 (Micro):    {f1_micro:.4f}\n",
    "Precision (Macro): {prec_macro:.4f}\n",
    "Recall (Macro):    {recall_macro:.4f}\n",
    "\n",
    "Classification Report:\n",
    "{report}\n",
    "\"\"\"\n",
    "\n",
    "print(metrics_text)\n",
    "with open(os.path.join(save_dir, \"metrics.txt\"), \"w\") as f:\n",
    "    f.write(metrics_text)\n",
    "\n",
    "# === Confusion Matrix ===\n",
    "plt.figure(figsize=(6, 5))\n",
    "cm = confusion_matrix(true_labels, ensemble_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix (Level 3 Ensemble)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "# === Platform-wise Evaluation\n",
    "print(\"\\nPlatform-wise Evaluation:\")\n",
    "platforms = [\"youtube\", \"reddit\", \"twitter\"]\n",
    "\n",
    "for platform in platforms:\n",
    "    mask = val_df['source'].str.lower() == platform\n",
    "    y_true = val_df[\"level_3_enc\"].values[mask]\n",
    "    y_pred = val_df[\"preds\"].values[mask]\n",
    "\n",
    "    print(f\"\\nPlatform: {platform.upper()}\")\n",
    "    if len(y_true) == 0:\n",
    "        print(f\"No samples for {platform.upper()}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"F1 Weighted: {f1_score(y_true, y_pred, average='weighted'):.4f}\")\n",
    "    print(f\"F1 Macro:    {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
    "\n",
    "    try:\n",
    "        platform_report = classification_report(\n",
    "            y_true, y_pred,\n",
    "            labels=list(range(len(le3.classes_))),\n",
    "            target_names=[str(c) for c in le3.classes_],\n",
    "            digits=4\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Error generating report:\", e)\n",
    "        platform_report = classification_report(y_true, y_pred, digits=4)\n",
    "\n",
    "    print(f\"Platform Report:\\n{platform_report}\")\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {save_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO+pLcyrWVfv1RrscxBwFkC",
   "mount_file_id": "1KO5HmL205Vy1RFfBBLI64wOFjPf2Mvlm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "026946106a0a4898a4ed27af1f90972a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08d57cf75def4f769b8e3dfdfbd5d185": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ec7e1d958f448bebf51382d090a72d3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d349dde5e4214e308c4e30902fe26eb9",
      "value": "tokenizer_config.json:‚Äá100%"
     }
    },
    "09a723d805f9480f8e3be5f40c599a44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c1c2da471444c589ace962b66e92658": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38a1725cba214785b84ed19c544b1b46",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_fe0785eeda2c4aaf8b96fa35fd67a7ce",
      "value": "‚Äá578/578‚Äá[00:00&lt;00:00,‚Äá29.8kB/s]"
     }
    },
    "0c3d13a6100a4156ae0cb7fe2850867d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8be996f677e46849e0f0157dd390c66",
      "max": 578,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ac2577a08be24dea9ffbded4a50fe8f9",
      "value": 578
     }
    },
    "2b83c235cbb04254b8a53805ceec4835": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38a1725cba214785b84ed19c544b1b46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3cf72ffaa4e346f8b116bc6984decd44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3e4b37007338486f9a1bc42d92e66676": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e771d40c267645dcb2033ac56f2bdf25",
       "IPY_MODEL_0c3d13a6100a4156ae0cb7fe2850867d",
       "IPY_MODEL_0c1c2da471444c589ace962b66e92658"
      ],
      "layout": "IPY_MODEL_9972a70aad4045f6b7fa063f316a5f0e"
     }
    },
    "4d914ae7555b4ddd9748393ce7c1b9f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6ec7e1d958f448bebf51382d090a72d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "721e119e9c664d1492ff38a7b2ec8752": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c3cdc5c057d247ddb2086a129cbb7a84",
       "IPY_MODEL_fdb89591f9be4739ba5ddb26470437d3",
       "IPY_MODEL_f415f2171d814452bd3fa9c3bab8b70d"
      ],
      "layout": "IPY_MODEL_026946106a0a4898a4ed27af1f90972a"
     }
    },
    "7ff248e227534f1fb063383982171076": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8760dd9c441b4477be9a56ae2e051964": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8dd65ceaca524e38b96e43f29b25930d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "935630bf0cef48418db6b4b28fb32475": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97bba8001ec64367ade6f56ec2cfef80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9972a70aad4045f6b7fa063f316a5f0e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9fc47a2f2a184ff5be80bd5f26d29aaa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_08d57cf75def4f769b8e3dfdfbd5d185",
       "IPY_MODEL_a90b4e5d1b114d5fa38248d03bce3bb5",
       "IPY_MODEL_fc36432f67324730aaad16eaed80fd3b"
      ],
      "layout": "IPY_MODEL_2b83c235cbb04254b8a53805ceec4835"
     }
    },
    "a8be996f677e46849e0f0157dd390c66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a90b4e5d1b114d5fa38248d03bce3bb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa8158d2f0034e88b91112a0d59c39e5",
      "max": 52,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3cf72ffaa4e346f8b116bc6984decd44",
      "value": 52
     }
    },
    "ac2577a08be24dea9ffbded4a50fe8f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c3cdc5c057d247ddb2086a129cbb7a84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d83ca0d931b2453eb53dcfabff88fcd8",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_8dd65ceaca524e38b96e43f29b25930d",
      "value": "spm.model:‚Äá100%"
     }
    },
    "cd4f66ca38504c1db54d327b501f3b2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d349dde5e4214e308c4e30902fe26eb9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d83ca0d931b2453eb53dcfabff88fcd8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df6801a544554ac18106282c0fc44511": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e771d40c267645dcb2033ac56f2bdf25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8760dd9c441b4477be9a56ae2e051964",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_4d914ae7555b4ddd9748393ce7c1b9f3",
      "value": "config.json:‚Äá100%"
     }
    },
    "f415f2171d814452bd3fa9c3bab8b70d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ff248e227534f1fb063383982171076",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_09a723d805f9480f8e3be5f40c599a44",
      "value": "‚Äá2.46M/2.46M‚Äá[00:00&lt;00:00,‚Äá33.5MB/s]"
     }
    },
    "fa8158d2f0034e88b91112a0d59c39e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc36432f67324730aaad16eaed80fd3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df6801a544554ac18106282c0fc44511",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_97bba8001ec64367ade6f56ec2cfef80",
      "value": "‚Äá52.0/52.0‚Äá[00:00&lt;00:00,‚Äá1.99kB/s]"
     }
    },
    "fdb89591f9be4739ba5ddb26470437d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_935630bf0cef48418db6b4b28fb32475",
      "max": 2464616,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd4f66ca38504c1db54d327b501f3b2a",
      "value": 2464616
     }
    },
    "fe0785eeda2c4aaf8b96fa35fd67a7ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
